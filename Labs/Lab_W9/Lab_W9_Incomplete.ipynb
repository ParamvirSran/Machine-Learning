{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The impact of regularization\n",
    "\n",
    "This part highlights the impact of using Ridge Regularization. The required imports are shown in the following cell. The random seed has been set to 42 for reproducibility. Some stylistic settings have been activated to improve the presentation of the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "#sns.set_context('paper')\n",
    "#plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first step is to generate two sets of data. The premise of this exercise is to mimic a real-world situation where the relationship between input and output is affected by noise. To do this, we will add Gaussian noise to a sample of 10 data points from the function y = sin(2πx). To visualize the effect of the noise on the data points, we also generate 100 data points for the continuous function y = sin(2πx)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate datapoints for y = sin (2πx) + noise\n",
    "x = np.random.random_sample(10)\n",
    "y_orig = np.sin(2*math.pi*x) \n",
    "noise = np.random.normal(0,0.3,10)\n",
    "y_noise = np.sin(2*math.pi*x) + noise\n",
    "\n",
    "#Generate Curve for y = sin (2πx)\n",
    "x2 = np.linspace(0,1,100)\n",
    "y2 = np.sin(2*math.pi*x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Below is the visualization of the original function (orange) along with the noisy datapoints (blue). A low number of data points was selected to clearly demonstrate how fitting regression models with low amounts of data leads to the manifestation of over and under fitting</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x2,y2, label = 'y = sin (2πx)', color='orange')\n",
    "plt.scatter(x, y_noise, label = 'y = sin (2πx) + noise ')\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel('y')\n",
    "plt.title('y = sin (2πx) vs. y = sin (2πx) + noise ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For our first model, we will be using linear regression with polynomial features, effectively making a polynomial regression model. The first stage in creating this model is to create the polynomial features. As each data point currently contains an x and y value, the 9 more features are generated by raising the x value to the power of 2, 3, ... 10.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate polynomial features up to degree 10\n",
    "data = pd.DataFrame(x, columns = ['x'])   ## These data points will be using to train the model\n",
    "for i in range(2,11):  \n",
    "    colname = 'x_%d'%i      \n",
    "    data[colname] = data['x']**i\n",
    "\n",
    "Test_Data = np.linspace(np.sort(x)[0], np.sort(x)[-1], num =50)   # 50 data points that will be used to demonstrate the relationship built by the model (extreme case) \n",
    "Test_Data = pd.DataFrame(Test_Data, columns = ['x'])\n",
    "for i in range(2,11): \n",
    "    colname = 'x_%d'%i \n",
    "    Test_Data[colname] = Test_Data['x']**i\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The following code creates 10 subplots, one for each of the polynomial degrees. For each degree, only the associated polynomial terms are used to build the linear regression model (ex. a polynomial degree of 4 uses 4 terms: x1, x2, x3, x4). As seen in these figures, the first two degrees show underfitting whereas degrees 7+ show clear signs of overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []  ## Python list\n",
    "rss = []\n",
    "fig, axs = plt.subplots(2,5, figsize = (25,12.5))\n",
    "for i in range(0,2):\n",
    "    for j in range (0,5):\n",
    "        LeastSquaresModel = Pipeline([('scaling', StandardScaler()),\n",
    "                                      ('linreg', LinearRegression())])\n",
    "        LeastSquaresModel.fit(data.iloc[:,0:5*i+j+1], y_noise)\n",
    "        Test_Data_pred_curve = LeastSquaresModel.predict(Test_Data.iloc[:,0:5*i+j+1]) ## we applied the model to the 50 data  points\n",
    "        y_pred_points = LeastSquaresModel.predict(data.iloc[:,0:5*i+j+1])  ## prediciting of the training data points\n",
    "        rss.append(np.sum(np.square(y_noise - y_pred_points))) ## Training RSS\n",
    "        coefs.append(LeastSquaresModel[1].coef_)  ## Model Coefficients\n",
    "        axs[i,j].plot(x2,y2, label = 'y = sin (2πx)', color='orange') ## the sin function\n",
    "        axs[i,j].plot(x, y_noise,\"o\", color='b') # the 10 training points (with noise)\n",
    "        axs[i,j].plot(Test_Data.x, Test_Data_pred_curve, color='g')  ## visulization of the model\n",
    "        axs[i,j].title.set_text(\"n = \" + str(5*i+j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As seen above, as the model complexity increases, so too does the level of overfitting. To further illustrate a consequence of the higher-order polynomials, we will explore the coefficients of each model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Size of Coefficients\n",
    "coef_mat = pd.DataFrame(coefs)  ## coefficent matrix\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_mat.index.name = 'polynomial rank'\n",
    "coef_mat.columns = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n",
    "coef_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>What we have observed above is known as coefficient explosion (magnitude of coefficients increases exponentially as model complexity increases) and is a key indication of overfitting. To further verify this, we will plot the Residual Sum of Squares (RSS) for each of the polynomial degrees. As seen in this plot, the highest rank polynomials have an RSS of 0 as they fit each point.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rss)\n",
    "plt.xlabel('Polynomial Rank')\n",
    "plt.ylabel('Residual Sum of Sqaures Loss')\n",
    "plt.title(\"The effect of Polynomial Rank on RSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To illustrate coefficient explosion, we plot the mean absolute magnitude of the coefficients for each of the polynomial ranks. As seen, the average magnitude of the coefficient drastically increases with model complexity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(coef_mat).mean(axis=1).plot(logy=True)\n",
    "plt.ylabel(\"Mean Absolute Magnitude of Coefficients\")\n",
    "plt.title(\"The effect of model complexity on the mean absolute magnitude of coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mitigate the effect of overfitting with respect to polynomial rank, Ridge Regression is used. As you know, in terms of Ridge Regression, the α parameter defines the level of regularization; an α of 0 indicates no regularization whereas an $α$ approaching $\\infty $ indicates full regularization and reduces all coefficients to 0. The following code illustrates the effect of this regularization on the 10 term polynomial developed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs = []\n",
    "rss = []\n",
    "fig, axs = plt.subplots(2,5, figsize = (25,12.5))\n",
    "alphas = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 100]\n",
    "for i in range(0,2):\n",
    "    for j in range (0,5):\n",
    "        RidgeModel = Pipeline([('scaling', StandardScaler()),\n",
    "                               ('ridge_reg', Ridge(alpha = alphas[5*i+j]))])\n",
    "        RidgeModel.fit(data, y_noise)  # training the model using the 10 data points \n",
    "        TestData_pred_curve = RidgeModel.predict(Test_Data) ## Model applied to the test data\n",
    "        y_pred_points = RidgeModel.predict(data)  ## Model applied on the traing data\n",
    "        rss.append(np.sum(np.square(y_noise - y_pred_points))) ## training RSS\n",
    "        coefs.append(RidgeModel[1].coef_)\n",
    "        axs[i,j].plot(x2,y2, label = 'y = sin (2πx)', color='orange') ## the sin function\n",
    "        axs[i,j].plot(x, y_noise,\"o\" , color='b') # plot the 10 training data points\n",
    "        axs[i,j].plot(Test_Data.x, TestData_pred_curve, color = 'g') # plot the predicted ouput of the 50 test data points\n",
    "        axs[i,j].title.set_text(\"α = \" + str(alphas[5*i+j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As seen above, as the value of α increases, there is a transition from overfitting to underfitting. This shows the power of regression to reverse the effects of overfitting but also highlights the importance of appropriately affecting the α value as too high of a value results in severe underfitting. We can see the effect through the size of coefficients for the various values of α below. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_mat = pd.DataFrame(coefs)\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_mat.index = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 100]\n",
    "coef_mat.index.name = 'alpha'\n",
    "coef_mat.columns = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n",
    "coef_mat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a visualization of the ridge where we can see the size of the coefficients drastically dropping and approaching zero as α approaches $\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(coef_mat).mean(axis=1).plot(logy=True)\n",
    "plt.ylabel(\"Mean Absolute Magnitude of Coefficients\")\n",
    "plt.title(\"The effect of regularization term α on the mean absolute magnitude of coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature expansion and regularization\n",
    "\n",
    "This part highlights feature expansion and the different regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial expansion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a data set and look at it.\n",
    "D=pd.read_csv('regression_data.csv')\n",
    "plt.plot(D.x, D.y, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a design matrix with polynomial expansion on X\n",
    "x=\n",
    "x=\n",
    "poly = \n",
    "X = \n",
    "\n",
    "\n",
    "plt.imshow(X, cmap='gray') # drop the cmap flag to get color\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "XDF = pd.DataFrame(X)\n",
    "XDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit a standard linear model to the data and plot prediction \n",
    "reg = \n",
    "reg\n",
    "yp=\n",
    "\n",
    "print(reg)\n",
    "\n",
    "plt.plot(x, D.y, 'k.', x, yp, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This seems very wiggly.\n",
    "# Could we do better with Ridge regression? \n",
    "# Let's regularize a lot\n",
    "print('Coef. matrix shape (%i x 1)' % reg.coef_.shape)\n",
    "\n",
    "ridge = \n",
    "ridge\n",
    "ypp=\n",
    "\n",
    "plt.plot(x,D.y,'k.',x,yp,'r-',x,ypp,'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the intercept problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happened? \n",
    "# The problem is that the Ridge coefficient was also applied to the intercept\n",
    "# Sometimes this is desired, sometimes not. (Usually not though)\n",
    "# In this case we do not want to include the intercept \n",
    "# into the regressors that should be regularized \n",
    "poly = \n",
    "X = \n",
    "scaler = \n",
    "\n",
    "X = \n",
    "\n",
    "pd.DataFrame(X).head()\n",
    "plt.imshow(X, cmap='gray')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we set fit_intercept = True (default), \n",
    "# ridge regression fits the intercept \n",
    "ridge = \n",
    "ridge\n",
    "ypp=\n",
    "\n",
    "plt.plot(x,D.y,'k.',x,yp,'r-',x,ypp,'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also redo the linear regression \n",
    "reg = skl.LinearRegression(fit_intercept=True)\n",
    "reg.fit(X,D.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now inspect the coefficients: No explicit intercept is fitted - ridge coefficients are smaller \n",
    "(reg.coef_,ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set the regularization coefficent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, how should we tune the regularization coefficient? \n",
    "# Let's use crossvalidation \n",
    "cv_scores = \n",
    "-cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematically vary the ridge coeficient on a log-scale\n",
    "lam = np.exp(np.linspace(-4,2,10))\n",
    "mse = np.zeros(10)\n",
    "\n",
    "for i in range(lam.size):\n",
    "    cv_scores = \n",
    "    mse[i]=-cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine lowest value \n",
    "plt.scatter(np.log(lam),mse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So low let's look at the crossvalidation error for the best setting of lambda \n",
    "cv_scores = \n",
    "\n",
    "print('CV score for alpha=exp(0.8): %.3f' % -cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las = \n",
    "las.\n",
    "yl=\n",
    "\n",
    "plt.plot(x, D.y, 'k.', label='_nolegend_')\n",
    "plt.plot(x, ypp, 'r-', label='Ridge')\n",
    "plt.plot(x, yl, 'b-', label='LASSO')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the coefficients. \n",
    "# What do you notice compared to the ridge? \n",
    "pd.DataFrame(\n",
    "\n",
    "# Ridge shrinks all coefficients towards zero; the lasso tends to give\n",
    "# a set of zero coefficients and leads to a sparse solution.\n",
    "# Note that for both ridge regression and the lasso the regression coefficients\n",
    "# can move from positive to negative values as they are shrunk toward zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a full path for Lasso\n",
    "eps = 5e-3 # The smaller eps, the longer the path  \n",
    "lambda_lasso, coefs_lasso, _ = \n",
    "\n",
    "print(f'minimum regularization parameter : %.3f'% np.amin(lambda_lasso))\n",
    "print(f'maximum regularization parameter : %.3f' % np.amax(lambda_lasso))\n",
    "#plt.plot(lambda_lasso)\n",
    "\n",
    "\n",
    "colors = ['b', 'r', 'g', 'c', 'k','c']\n",
    "neg_log_lambda = \n",
    "\n",
    "for i in range(6):\n",
    "    l1 = plt.plot(neg_log_lambda, coefs_lasso[i,], c=colors[i])\n",
    "\n",
    "plt.xlabel('Neg. Log. Lambda')\n",
    "plt.ylabel('Coef.')\n",
    "plt.legend(['Coef. 1', 'Coef. 2', 'Coef. 3',\n",
    "           'Coef. 4', 'Coef. 5', 'Coef. 6'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet (and automatic search for best hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ElasticNet object\n",
    "ElasticNet = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n",
    "ElasticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best coefficients\n",
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fit\n",
    "yen=\n",
    "\n",
    "plt.plot(x, D.y, 'k.', label='_nolegend_')\n",
    "plt.plot(x, ypp, 'r-', label='Ridge')\n",
    "plt.plot(x, yl, 'b-', label='LASSO')\n",
    "plt.plot(x, yen, 'g-.', label='ElasticNet')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "pd.DataFrame("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
