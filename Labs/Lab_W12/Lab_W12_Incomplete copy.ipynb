{"cells":[{"cell_type":"markdown","metadata":{"id":"X8wR38XT5FFC","tags":[]},"source":["# Clustering\n"]},{"cell_type":"markdown","metadata":{"id":"BE-coXaN5FFD"},"source":["# Global Toolbox"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2541,"status":"ok","timestamp":1679666047062,"user":{"displayName":"Alireza Fazeli","userId":"13019604121988176443"},"user_tz":240},"id":"n6g3EM9C5FFE"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.cluster import AgglomerativeClustering, KMeans\n","from sklearn import datasets\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","# !pip install yellowbrick\n","from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n","from yellowbrick.cluster.elbow import kelbow_visualizer\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import seaborn as sns\n","\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","seed = 202321\n","np.random.seed(seed)\n","\n","%config InlineBackend.figure_format = 'retina'\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwAz3CDp5FFF"},"outputs":[],"source":["# Load iris datset, set 'species' to be categorical variable containing class labels\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n","                    columns= iris['feature_names'] + ['target'])\n","df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n","df.drop('target', axis='columns',inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_a0_vkFH5FFG","outputId":"ceb141d1-1f97-4182-efdf-29a6086434e4"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"5CaCVGhY5FFH"},"source":["# Exploring and Evaluating Clusters"]},{"cell_type":"markdown","metadata":{"id":"QYwGPicg5FFH"},"source":["## K-means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQ2NYzkt5FFH","outputId":"5fcdd2b8-bb33-4572-cb61-b5c7df3dbc0f"},"outputs":[],"source":["# Initializing the cluster algorithm\n","KClusterer = \n","\n","\n","# Fit the cluster and predict the label in one step. Calls any preprocessing step plus model\n","df['cluster_label'] = \n","\n","# Create a pairplot using seaborn\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dFRn4GOG5FFJ"},"source":["Now we'll use the elbow method to determine the optimal number of clusters. For this, we will use the package [yellowbrick](https://www.scikit-yb.org/en/latest/quickstart.html), which specializes in visualization of data science models (give it a read, it's pretty interesting!). In particular we will use the [KelbowVisualizer](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html) function. By default, it will give us the location of the elbow using the [kneed](https://github.com/arvkevi/kneed) algorithm. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTtCV6Ls5FFJ","outputId":"d2a0ba94-7f38-42aa-9441-db28313d6791"},"outputs":[],"source":["# Initialize the object\n","visualizer = "]},{"cell_type":"markdown","metadata":{"id":"jGbwfPlh5FFK"},"source":["The method suggests it's actually four cluster the optimal number. Let's test this theory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTBWenEo5FFL","outputId":"b01ffe79-3c37-4ee9-b144-58df4c667122"},"outputs":[],"source":["# Initializing the cluster algorithm\n","KClusterer = \n","\n","\n","# Fit the cluster and predict the label in one step. Calls any preprocessing step plus model\n","df['cluster_label'] = \n","\n","# Create a pairplot using seaborn\n","sns.pairplot(vars=iris['feature_names'], # Variable names   \n","             hue='cluster_label',        # How to colour the points. Use cluster labels\n","             markers=['X','o','^', 's'],      # Differentiate markers\n","             data=df                     # What data to use\n","            )\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1FwKJ9aI5FFL"},"source":["It does look better, no? This is a simple way to determine the number of clusters in your models!"]},{"cell_type":"markdown","metadata":{"id":"YhEtrE7s5FFM"},"source":["## Agglomerative"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQPwtrfQ5FFM","outputId":"24bf4d5f-0512-44b4-e2b3-4bd6e5ad3a3b"},"outputs":[],"source":["# Now a pipeline with the agglomerative cluster.\n","AgglomerativeIris = \n","\n","df['cluster_label'] = \n","\n","sns.pairplot(vars=iris['feature_names'], \n","             hue='cluster_label',\n","             markers=['X','o','^','s'],\n","             data=df,\n","             diag_kind='hist'            # What plot to use for diagonal. kde causes issues in single linkage due to too few points\n","            )\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cAj3Kk1Z5FFM"},"source":["Four does not seem to be the optimal number of clusters for this model! You could use the elbow method, but you need to implement it on your own. The alternative is the silhouette method (see below)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1D55OKB5FFM","outputId":"b7bf09ad-ea1d-4f10-9861-83ebfd836d44"},"outputs":[],"source":["# Now a pipeline with the agglomerative cluster. \n","cl_pipe = \n","\n","df['cluster_label'] = \n","\n","sns.pairplot(vars=iris['feature_names'], \n","             hue='cluster_label',\n","             markers=['X','o','^','s'],\n","             data=df,\n","             diag_kind='kde'            # What plot to use for diagonal.\n","            )\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqNVK6HJ5FFN","outputId":"86f78dc2-af07-445a-da8b-b621283bc181"},"outputs":[],"source":["# Now a pipeline with the agglomerative cluster. \n","cl_pipe = Pipeline([\n","    ('aggcl',AgglomerativeClustering(n_clusters=4,         # Number of clusters\n","                                     affinity='euclidean', # Type of distance. Depends on your data and you can create your own!\n","                                     linkage=\"average\"      # Type of linkage. Options \n","                                    )\n","    )\n","])\n","\n","df['cluster_label'] = cl_pipe.fit_predict(X)\n","\n","sns.pairplot(vars=iris['feature_names'], \n","             hue='cluster_label',\n","             markers=['X','o','^','s'],\n","             data=df,\n","             diag_kind='kde'            # What plot to use for diagonal.\n","            )\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BWnHlWc_5FFO"},"source":["## Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jdAdYwcn5FFO","outputId":"9f0d958a-6d24-4903-b79f-c5bce3f83896"},"outputs":[],"source":["# Now we create a proper pipeline with scaling and a model\n","cl_pipe = \n","\n","# Again we fit and predict\n","df['cluster_label_scaled'] = cl_pipe.fit_predict(X)\n","\n","# And we plot\n","sns.pairplot(vars=iris['feature_names'],\n","             hue='cluster_label_scaled',\n","             data=df,\n","             diag_kind='hist')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iSTiUlXi5FFO"},"source":["# Some synthetic data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2Sa9AIc5FFP","outputId":"fb0918a7-aad2-4acd-cea0-e61e6cc454a0"},"outputs":[],"source":["centers = [(0,0,0),(0,0,1),(0,1,0),(0,1,1),\n","           (1,0,0),(1,0,1),(1,1,0),(1,1,1)]\n","X, y = datasets.make_blobs(n_samples=400, centers=centers, \n","           cluster_std=0.25, shuffle=True, random_state=0)\n","\n","synth_df = pd.DataFrame(X)\n","sns.scatterplot(data=X)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pYoEOJD5FFP","outputId":"59c5dd6f-2c58-4953-fa02-7cc8aeac438b"},"outputs":[],"source":["acl_pipe = Pipeline([\n","    ('scale', StandardScaler()),\n","    ('aggcl', AgglomerativeClustering(n_clusters=8, linkage=\"average\"))\n","])\n","\n","synth_df['cluster_label_scaled'] = acl_pipe.fit_predict(X)\n","\n","print('The silhouette score is equal to %.3f' % silhouette_score(X, synth_df['cluster_label_scaled']))\n","\n","sns.pairplot(hue='cluster_label_scaled', vars=[0,1,2], data=synth_df)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTlXdW9N5FFQ","outputId":"2447c057-68fa-43ea-88a8-c5f61d6fc775"},"outputs":[],"source":["fig = plt.figure(figsize=(4, 4))\n","ax = Axes3D(fig, elev=60, azim=134, auto_add_to_figure=False)\n","fig.add_axes(ax)\n","ax.scatter(synth_df.iloc[:, 0], synth_df.iloc[:, 1], synth_df.iloc[:, 2],\n","           c=synth_df['cluster_label_scaled'], edgecolor='k')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_HAUckA25FFQ"},"source":["# Silhouette Plot Example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zjCqLZt5FFQ"},"outputs":[],"source":["# Make some complex data\n","X, y = datasets.make_moons(n_samples=400, random_state=seed)\n","\n","synth_df = pd.DataFrame(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gegU_SdI5FFQ","outputId":"a59d9055-6870-4a3c-a88d-dadd55467aad"},"outputs":[],"source":["# Create a pipeline\n","acl_pipe = Pipeline([\n","    ('scale', StandardScaler()),\n","    ('aggcl', AgglomerativeClustering(n_clusters=2, linkage=\"single\"))\n","])\n","\n","# Fit it\n","synth_df['cluster_label_scaled'] = acl_pipe.fit_predict(X)\n","\n","# Calculate the silhouette score\n","agg_sil_avg = silhouette_score(X, synth_df['cluster_label_scaled'])\n","print('The silhouette score for an Agglomerative Cluster is %.3f' % agg_sil_avg)\n","\n","# plot the output\n","sns.pairplot(hue='cluster_label_scaled', vars=[0,1], data=synth_df)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3VH4Qar5FFS","outputId":"3c42ebeb-af41-4f8c-9705-fa3c10ce5fbf"},"outputs":[],"source":["# Repeat, but now using k-means\n","kmeans_pipe\n","\n","kmeans_pipe\n","\n","synth_df['cluster_label_scaled'] =\n","\n","kmeans_sil_avg = silhouette_score\n","\n","print('The silhouette score for K-means is %.3f' % kmeans_sil_avg)\n","\n","# We can plot a silhouette plot for k-means using yellowbricks\n","visualizer = SilhouetteVisualizer(kmeans_pipe[1]) # Get the kmeans model\n","visualizer.fit(kmeans_pipe[0].transform(X)) # Pass the scaled data\n","visualizer.show() # show the plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gppI8PuX5FFS","outputId":"f75cb1ff-784f-4999-b430-4a71d851c6c4"},"outputs":[],"source":["# We also plot the pairplot\n","sns.pairplot(hue='cluster_label_scaled', vars=[0,1], data=synth_df)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CwyB5W8i5FFS"},"source":["As yellowbricks does not (yet) allow for agglomerative clustering models, we need to manually calculate the silhouette scores. The following code does that and compares both."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmCbRT3t5FFT","outputId":"b38cec74-0824-41f4-c54d-dbd493e7b23c"},"outputs":[],"source":["# Create a full silhoutte plot for the models\n","\n","# Initialize the plot. Two plots, side-by-side, stores in ax1 and ax2.\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","n_clusters = 2\n","\n","# Set limits. Score can go between -1, 1 but they sometimes can be trimmed.\n","ax1.set_xlim([-1, 1])\n","ax2.set_xlim([-1, 1])\n","\n","# The (n_clusters+1)*10 is for inserting blank space between silhouette\n","# plots of individual clusters, to demarcate them clearly.\n","ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n","ax2.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n","\n","# Initialize the clusterer with n_clusters value and a random generator\n","# seed of 10 for reproducibility.\n","agg_cluster_labels = acl_pipe.fit_predict(X)\n","kmeans_cluster_labels = kmeans_pipe.fit_predict(X)\n","\n","\n","# Compute the silhouette scores for each sample\n","sample_silhouette_values_agg = silhouette_samples(X, agg_cluster_labels)\n","sample_silhouette_values_kmeans = silhouette_samples(X, kmeans_cluster_labels)\n","\n","y_lower = 10\n","\n","# Iterate over the clusters - Agglomerative\n","for i in range(n_clusters):\n","    # Aggregate the silhouette scores for samples belonging to\n","    # cluster i, and sort them\n","    ith_cluster_silhouette_values = \\\n","        sample_silhouette_values_agg[agg_cluster_labels == i]\n","\n","    ith_cluster_silhouette_values.sort()\n","\n","    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = cm.nipy_spectral(float(i) / n_clusters)\n","    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                      0, ith_cluster_silhouette_values,\n","                      facecolor=color, edgecolor=color, alpha=0.7)\n","\n","    # Label the silhouette plots with their cluster numbers at the middle\n","    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","    # Compute the new y_lower for next plot\n","    y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","ax1.set_title(\"Silhouette plot \\n Agglomerative Cluster model\")\n","ax1.set_xlabel(\"Silhouette coefficient values\")\n","ax1.set_ylabel(\"Cluster Label\")\n","\n","# The vertical line for average silhouette score of all the values\n","ax1.axvline(x=agg_sil_avg, color=\"red\", linestyle=\"--\")\n","\n","ax1.set_yticks([])\n","\n","# Iterate over the clusters - K-means\n","y_lower = 10\n","for i in range(n_clusters):\n","    # Aggregate the silhouette scores for samples belonging to\n","    # cluster i, and sort them\n","    ith_cluster_silhouette_values = \\\n","        sample_silhouette_values_kmeans[kmeans_cluster_labels == i]\n","\n","    ith_cluster_silhouette_values.sort()\n","\n","    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = cm.nipy_spectral(float(i) / n_clusters)\n","    ax2.fill_betweenx(np.arange(y_lower, y_upper),\n","                      0, ith_cluster_silhouette_values,\n","                      facecolor=color, edgecolor=color, alpha=0.7)\n","\n","    # Label the silhouette plots with their cluster numbers at the middle\n","    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","    # Compute the new y_lower for next plot\n","    y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","ax2.set_title(\"Silhouette plot \\n K-Means Cluster model\")\n","ax2.set_xlabel(\"Silhouette coefficient values\")\n","ax2.set_ylabel(\"Cluster Label\")\n","\n","# The vertical line for average silhouette score of all the values\n","ax2.axvline(x=kmeans_sil_avg, color=\"red\", linestyle=\"--\")\n","\n","ax2.set_yticks([])  \n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5rVAETU25FFT"},"source":["# Classification vs Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXUKIfte5FFT","outputId":"c8fd19a6-8a6b-46a5-a684-8d9e99c08f5a"},"outputs":[],"source":["data = datasets.load_iris()\n","X = data.data\n","y = data.target\n","data.target_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3LzkmT65FFT","outputId":"3ba0f9a9-1941-4993-b572-dd9bc5115a7b"},"outputs":[],"source":["plt.figure(figsize=(9, 3.5))\n","\n","plt.subplot(121)\n","plt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\n","plt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\n","plt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(fontsize=12)\n","\n","plt.subplot(122)\n","plt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.tick_params(labelleft=False)\n","\n","plt.show()\n","# Eye would see only 2 clusters but once color code by labels there are actually 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H54kjz2J5FFU","outputId":"624b1e45-f216-43a0-9325-884f57a5354f"},"outputs":[],"source":["# GaussianMixture is just an FYI and not an official part of the course\n","\n","# A Gaussian mixture model can actually separate the 3 clusters pretty well\n","# (using all 4 features: petal length, petal width, sepal length, and sepal width)\n","\n","from sklearn.mixture import GaussianMixture\n","\n","y_pred = GaussianMixture(n_components=3, random_state=seed).fit(X).predict(X)\n","\n","# Let's map each cluster to a class. Instead of hard coding the mapping,\n","# we will pick the most common class for each cluster (using the `scipy.stats.mode()` function):\n","from scipy import stats\n","\n","mapping = {}\n","for class_id in np.unique(y):\n","    mode, _ = stats.mode(y_pred[y==class_id], keepdims=True)\n","    mapping[mode[0]] = class_id\n","\n","y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])\n","\n","# plotting\n","plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], \"yo\", label=\"Cluster 1\")\n","plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], \"bs\", label=\"Cluster 2\")\n","plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], \"g^\", label=\"Cluster 3\")\n","plt.xlabel(\"Petal length\", fontsize=14)\n","plt.ylabel(\"Petal width\", fontsize=14)\n","plt.legend(loc=\"upper left\", fontsize=12)\n","plt.show()\n","\n","print('Number of total instances:', len(y_pred))\n","print('Number of correct assignments:',np.sum(y_pred==y))\n","print('Correct assignment percentage:', (np.sum(y_pred==y) / len(y_pred)).round(2)*100)\n","\n","# End of FYI."]},{"cell_type":"markdown","metadata":{"id":"OZZtbKZu5FFU"},"source":["## K-Means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7jA1Gw45FFU","outputId":"0c1c3a0f-745f-4406-ba31-3a649a43edd9"},"outputs":[],"source":["# Let's start by generating some blobs and plot them:\n","from sklearn.datasets import make_blobs\n","blob_centers = np.array(\n","    [[ 0.2,  2.3],\n","     [-1.5 ,  2.3],\n","     [-2.8,  1.8],\n","     [-2.8,  2.8],\n","     [-2.8,  1.3]])\n","blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n","\n","X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=seed)\n","\n","# plotting\n","def plot_clusters(X, y=None):\n","    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n","    plt.xlabel(\"$x_1$\", fontsize=14)\n","    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n","plt.figure(figsize=(8, 4))\n","plot_clusters(X)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eu78CscT5FFV","outputId":"390f377d-4b9e-45c9-fd03-0d06f167ef92"},"outputs":[],"source":["# Fit and predict\n","# Let's train a K-Means clusterer on this dataset.\n","# It will try to find each blob's center and assign each instance to the closest blob:\n","\n","k = 5\n","kmeans = \n","y_pred = \n","\n","print('Each instance was assigned to one of the k clusters', y_pred)\n","\n","# Nota Bene: In clustering, \"label\" is the index of the cluster that an instance gets assigned to by\n","# the algorithm (not to be mistaken with the other \"label\" which we know from supervised learning)\n","\n","# The KMeans instance preserves a copy of the labels of the\n","# instances it was trained on, available via the `.labels_` attribute\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3qBC3YH5FFV","outputId":"ba0945f3-138c-4904-b3f1-e9bf112bd97d"},"outputs":[],"source":["# And the following k centroids (i.e., cluster centers) were estimated:\n","kmeans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KXVZLFsw5FFV","outputId":"966744a1-4de0-4b41-bb8b-957de13af87f"},"outputs":[],"source":["# Let's predict the labels of some new instances:\n","X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n","kmeans."]},{"cell_type":"markdown","metadata":{"id":"cJouzOeZ5FFV"},"source":["## Decision Boundaries\n","\n","Let's plot the model's decision boundaries. This gives us a _Voronoi diagram_:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXHJ_tkE5FFV","outputId":"e8d5962b-bb75-4b85-d909-345acd49a0f4"},"outputs":[],"source":["def plot_data(X):\n","    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n","\n","def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n","    if weights is not None:\n","        centroids = centroids[weights > weights.max() / 10]\n","    plt.scatter(centroids[:, 0], centroids[:, 1],\n","                marker='o', s=35, linewidths=8,\n","                color=circle_color, zorder=10, alpha=0.9)\n","    plt.scatter(centroids[:, 0], centroids[:, 1],\n","                marker='x', s=2, linewidths=12,\n","                color=cross_color, zorder=11, alpha=1)\n","\n","def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True, show_xlabels=True, show_ylabels=True):\n","    mins = X.min(axis=0) - 0.1\n","    maxs = X.max(axis=0) + 0.1\n","    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution), np.linspace(mins[1], maxs[1], resolution))\n","    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","\n","    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap=\"Pastel2\")\n","    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k')\n","    plot_data(X)\n","    \n","    if show_centroids:\n","        plot_centroids(clusterer.cluster_centers_)\n","\n","    if show_xlabels:\n","        plt.xlabel(\"$x_1$\", fontsize=14)\n","    else:\n","        plt.tick_params(labelbottom=False)\n","    if show_ylabels:\n","        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n","    else:\n","        plt.tick_params(labelleft=False)\n","\n","plt.figure(figsize=(8, 4))\n","plot_decision_boundaries(kmeans, X)\n","plt.show()\n","\n","# Not bad! Some of the instances near the edges were probably\n","# assigned to the wrong cluster, but overall it looks pretty good."]},{"cell_type":"markdown","metadata":{"id":"ooOUXsux5FFW"},"source":["## Hard Clustering _vs_ Soft Clustering\n","\n","Instead of assigning each instance to a single cluster, which is called hard clustering, it can be useful to give each instance a score per cluster, which is called soft clustering, aka Fuzzy clustering, (i.e., data points can belong to more than one cluster). The score can be the distance between the instance and the centroids. In the KMeans class, the `transform()` method measures that. There are methods such as Weighted K-Means utilizing this to perform soft clustering. Also, if you have a high-dimensional\n","dataset and you transform it this way, you end up with a `k`-dimensional dataset that for certain use cases can be a very efficient nonlinear dimensionality reduction technique."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGCbJLRe5FFW","outputId":"21b5fa34-9a10-4c90-e9bf-62d42d3e1428"},"outputs":[],"source":["kmeans."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8wYl4xD5FFW","outputId":"48276e7d-fd79-4416-d998-263a7e163186"},"outputs":[],"source":["# You can verify that this is indeed the Euclidian distance between each instance and each centroid:\n","np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, axis=2)"]},{"cell_type":"markdown","metadata":{"id":"aGMoZS--5FFX"},"source":["## K-Means Algorithm\n","\n","The K-Means algorithm is one of the fastest clustering algorithms, and also one of the simplest:\n","* First initialize $k$ centroids randomly: $k$ distinct instances are chosen randomly from the dataset and the centroids are placed at their locations.\n","* Repeat until convergence (i.e., until the centroids stop moving):\n","    * Assign each instance to the closest centroid.\n","    * Update the centroids to be the mean of the instances that are assigned to them.\n","\n","\n","The `KMeans` class applies an optimized algorithm by default. To get the original K-Means algorithm (for educational purposes only), you must set `init=\"random\"`, `n_init=1` and `algorithm=\"full\"`.\n","\n","Let's run the K-Means algorithm for 1, 2 and 3 iterations, to see how the centroids move around:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UAtrh1m5FFX","outputId":"4719648f-c6eb-4028-9097-aa8b18f91b15"},"outputs":[],"source":["kmeans_iter1 = \n","kmeans_iter2 = \n","kmeans_iter3 = \n","\n","kmeans_iter1.fit(X)\n","kmeans_iter2.fit(X)\n","kmeans_iter3.fit(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1NIvCt05FFX","outputId":"31f73f4b-7774-47c2-df75-5c58285e4d46"},"outputs":[],"source":["# And let's plot this:\n","plt.figure(figsize=(10, 8))\n","\n","plt.subplot(321)\n","plot_data(X)\n","plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n","plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n","plt.tick_params(labelbottom=False)\n","plt.title(\"Update the centroids (initially randomly)\", fontsize=14)\n","\n","plt.subplot(322)\n","plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n","plt.title(\"Label the instances\", fontsize=14)\n","\n","plt.subplot(323)\n","plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\n","plot_centroids(kmeans_iter2.cluster_centers_)\n","\n","plt.subplot(324)\n","plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n","\n","plt.subplot(325)\n","plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n","plot_centroids(kmeans_iter3.cluster_centers_)\n","\n","plt.subplot(326)\n","plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YVBOpE2Z5FFY"},"source":["## K-Means Variability\n","\n","In the original K-Means algorithm, the centroids are just initialized randomly, and the algorithm simply runs a single iteration to gradually improve the centroids, as we saw above.\n","\n","However, one major problem with this approach is that if you run K-Means multiple times (or with different random seeds), it can converge to very different solutions, as you can see below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_KJ8RwZ5FFY"},"outputs":[],"source":["def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n","    clusterer1.fit(X)\n","    clusterer2.fit(X)\n","\n","    plt.figure(figsize=(10, 3.2))\n","\n","    plt.subplot(121)\n","    plot_decision_boundaries(clusterer1, X)\n","    if title1:\n","        plt.title(title1, fontsize=14)\n","\n","    plt.subplot(122)\n","    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n","    if title2:\n","        plt.title(title2, fontsize=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J47IHC1o5FFY","outputId":"b8705516-68c4-4684-fe5c-c62ede476235"},"outputs":[],"source":["kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, algorithm=\"lloyd\", random_state=2)\n","kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, algorithm=\"lloyd\", random_state=5)\n","\n","plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X, \"Solution 1\", \"Solution 2 (with a different random init)\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"N0X2TPd05FFZ"},"source":["## Finding Optimal `k`"]},{"cell_type":"markdown","metadata":{"id":"BbofIWaw5FFZ"},"source":["### Inertia Score (Elbow Method)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__iLNijD5FFZ","outputId":"f9a5cdd2-534d-4341-d9b7-8ca4acc88989"},"outputs":[],"source":["kmeans_k3 = KMeans(n_clusters=3, random_state=seed)\n","kmeans_k8 = KMeans(n_clusters=8, random_state=seed)\n","\n","plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qz647gEh5FFZ","outputId":"362a97c9-1deb-46e6-adbc-83d6556a34aa"},"outputs":[],"source":["print(kmeans_k3.inertia_)\n","print(kmeans_k8.inertia_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tM94fT9S5FFZ","outputId":"e1da465f-cc4c-48c8-872b-34946abf6f4e"},"outputs":[],"source":["# We cannot simply take the value of k that minimizes the inertia, since it keeps getting lower as we increase k. \n","# Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the\n","# lower the inertia will be. However, we can plot the inertia as a function of k and analyze the resulting curve:\n","kmeans_per_k = [KMeans(n_clusters=k, random_state=seed).fit(X) for k in range(2, 10)]\n","inertias = [model.inertia_ for model in kmeans_per_k]\n","\n","plt.figure(figsize=(8, 3.5))\n","plt.plot(range(2, 10), inertias, \"bo-\")\n","plt.xlabel(\"$k$\", fontsize=14)\n","plt.ylabel(\"Inertia\", fontsize=14)\n","plt.annotate('Elbow',\n","             xy=(4, inertias[3]),\n","             xytext=(0.55, 0.55),\n","             textcoords='figure fraction',\n","             fontsize=16,\n","             arrowprops=dict(facecolor='black', shrink=0.1)\n","            )\n","# plt.axis([1, 8.5, 0, 1300])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-29Ou3R5FFa","outputId":"c4daa893-83f5-4c04-a64d-515ed8c529de"},"outputs":[],"source":["# A simpler way to plot the same thing:\n","\n","from yellowbrick.cluster import KElbowVisualizer\n","\n","kmeans = KMeans(random_state=seed)\n","\n","visualizer = \n","\n","visualizer\n","visualizer\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xafT5c1u5FFa","outputId":"dedfdf8c-ca1a-425c-c625-79090d9a5d52"},"outputs":[],"source":["# As you can see, there is an elbow at k=4, which means that less clusters than that would be bad,\n","# and more clusters would not help much and might cut clusters in half. So k=4 is a pretty good choice.\n","# Of course in this example it is not perfect since it means that the two blobs in the lower left will\n","# be considered as just a single cluster, but it's a pretty good clustering nonetheless.\n","plot_decision_boundaries(kmeans_per_k[4-1], X)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ea2Wd7w35FFa"},"source":["### Silhouette score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44yK59715FFa","outputId":"06ff8886-45cb-45e4-c9bf-c645cb91124e"},"outputs":[],"source":["print()\n","\n","silhouette_scores = \n","\n","plt.figure(figsize=(8, 3))\n","plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n","plt.xlabel(\"$k$\", fontsize=14)\n","plt.ylabel(\"Silhouette score\", fontsize=14)\n","plt.axis([1.8, 8.5, 0.55, 0.7])\n","plt.show()\n","\n","# As you can see, this visualization is much richer than the inertia one:\n","# in particular, although it confirms that k=4 is a very good choice,\n","# but it also underlines the fact that k=5 is quite good as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LalYugTp5FFa","outputId":"3ac04c27-62ce-4e38-986e-2027d534179f"},"outputs":[],"source":["# We can make better judgment by looking at silhouette diagram\n","\n","from sklearn.metrics import silhouette_samples\n","from matplotlib.ticker import FixedLocator, FixedFormatter\n","\n","plt.figure(figsize=(11, 9))\n","\n","for k in (3, 4, 5, 6):\n","    plt.subplot(2, 2, k - 2)\n","    \n","    y_pred = kmeans_per_k[k - 1].labels_\n","    silhouette_coefficients = silhouette_samples(X, y_pred)\n","\n","    padding = len(X) // 30\n","    pos = padding\n","    ticks = []\n","    for i in range(k):\n","        coeffs = silhouette_coefficients[y_pred == i]\n","        coeffs.sort()\n","\n","        color = mpl.cm.Spectral(i / k)\n","        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n","                          facecolor=color, edgecolor=color, alpha=0.7)\n","        ticks.append(pos + len(coeffs) // 2)\n","        pos += len(coeffs) + padding\n","\n","    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n","    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n","    if k in (3, 5):\n","        plt.ylabel(\"Cluster\")\n","    \n","    if k in (5, 6):\n","        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","        plt.xlabel(\"Silhouette Coefficient\")\n","    else:\n","        plt.tick_params(labelbottom=False)\n","\n","    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n","    plt.title(\"$k={}$\".format(k), fontsize=16)\n","\n","plt.show()\n","\n","# As you can see, k=5 looks like the best option here, as all clusters are roughly the same size,\n","# and they all cross the dashed line, which represents the mean silhouette score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzaR5dMF5FFb","outputId":"b0f94de0-3ff5-4c92-d0e0-31c2f4e8cbf1"},"outputs":[],"source":["# getting them using SilhouetteVisualizer\n","\n","for k in np.arange(4, 6):\n"]},{"cell_type":"markdown","metadata":{"id":"8GTJ1poU5FFb"},"source":["## Limits of K-Means"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pdih623J5FFb","outputId":"863a0be8-2d50-490c-c373-0705308a7425"},"outputs":[],"source":["# Despite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw,\n","# it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need\n","# to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave\n","# very well when the clusters have varying sizes, different densities, or nonspherical shapes.\n","\n","# The example below shows how K-Means clusters a dataset containing three ellipsoidal clusters\n","# of different dimensions, densities, and orientations.\n","\n","X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=seed)\n","X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n","X2, y2 = make_blobs(n_samples=250, centers=1, random_state=seed)\n","X2 = X2 + [6, -8]\n","X = np.r_[X1, X2]\n","y = np.r_[y1, y2]\n","plot_clusters(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWHVA5Xs5FFc","outputId":"0603f2ca-59e1-4dbe-d97d-f92de80c7cfa"},"outputs":[],"source":["kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=seed)\n","kmeans_bad = KMeans(n_clusters=3, random_state=42)\n","kmeans_good.fit(X)\n","kmeans_bad.fit(X)\n","\n","\n","plt.figure(figsize=(10, 3.2))\n","\n","plt.subplot(121)\n","plot_decision_boundaries(kmeans_good, X)\n","plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n","\n","plt.subplot(122)\n","plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n","plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_AJqy7nN5FFc"},"source":["# Using Clustering for Image Segmentation\n","\n","Image segmentation is the task of partitioning an image into multiple segments. In semantic segmentation, all pixels that are part of the same object type get assigned to the same segment. For example, in a self-driving car’s vision system, all pixels that are part of a pedestrian’s image might be assigned to the \"pedestrian\" segment (there would be one segment containing all the pedestrians). In instance segmentation, all pixels that are part of the same individual object are assigned to the same segment. In this case there would be a different segment for each pedestrian. The state of the art in semantic or instance segmentation today is achieved using complex architectures based on convolutional neural networks. Here, we are going to do something much simpler: color segmentation. We will simply assign pixels to the same segment if they have a similar color. For example, if you want to analyze satellite images to measure how much total forest area there is in a region, color segmentation may be just fine."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDB7nZG15FFd","outputId":"6ce6702e-b634-4018-eb52-7b79acccf68b"},"outputs":[],"source":["# load the ladybug image\n","from matplotlib.image import imread\n","image = imread(\"./ladybug.png\")\n","image.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pB7NB8iC5FFe","outputId":"ff95f661-62fa-4ea9-b61c-a2028cb948f6"},"outputs":[],"source":["# The image is represented as a 3D array. The first dimension’s size is the height; the\n","# second is the width; and the third is the number of color channels, in this case red,\n","# green, and blue (RGB). In other words, for each pixel there is a 3D vector containing\n","# the intensities of red, green, and blue, each between 0.0 and 1.0 (or between 0 and\n","# 255, if you use `imread()`). Some images may have fewer channels, such as grayscale\n","# images (one channel). And some images may have more channels, such as images with an\n","# additional alpha channel for transparency or satellite images, which often contain\n","# channels for many light frequencies (e.g., infrared). The following code reshapes the\n","# array to get a long list of RGB colors, then it clusters these colors using K-Means:\n","\n","X = image.reshape(-1, 3)\n","kmeans = KMeans(n_clusters=8, random_state=seed).fit(X)\n","segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n","segmented_img = segmented_img.reshape(image.shape)\n","\n","segmented_imgs = []\n","n_colors = (10, 8, 6, 4, 2)\n","for n_clusters in n_colors:\n","    kmeans = KMeans(n_clusters=n_clusters, random_state=seed).fit(X)\n","    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n","    segmented_imgs.append(segmented_img.reshape(image.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hI0eNf0r5FFe","outputId":"71d83b98-8a68-4a82-e94a-f58a8c10689a"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.subplots_adjust(wspace=0.05, hspace=0.1)\n","\n","plt.subplot(231)\n","plt.imshow(image)\n","plt.title(\"Original image\")\n","plt.axis('off')\n","\n","for idx, n_clusters in enumerate(n_colors):\n","    plt.subplot(232 + idx)\n","    plt.imshow(segmented_imgs[idx])\n","    plt.title(\"{} colors\".format(n_clusters))\n","    plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"meeoDt0y5FFe"},"source":["# Using Clustering for Preprocessing\n","\n","Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm. As an example of using clustering for dimensionality reduction, let’s tackle the digits dataset, which is a simple MNIST-like dataset containing 1797 grayscale 8x8 images representing the digits 0 to 9. First, load the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUMNfwm65FFe","outputId":"b65c2f20-ca50-42e2-ecec-7688671f5d5e"},"outputs":[],"source":["from sklearn.datasets import load_digits\n","X_digits, y_digits = load_digits(return_X_y=True)\n","\n","# Let's split it into a training set and a test set:\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, random_state=seed)\n","\n","# Now let's fit a Logistic Regression model and evaluate it on the test set:\n","from sklearn.linear_model import LogisticRegression\n","\n","log_reg = \n","log_reg.fit(X_train, y_train)\n","\n","log_reg_score = \n","log_reg_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YD0sE0ce5FFj","outputId":"173ca20c-e7a7-4f5a-f6fe-062a86aef232"},"outputs":[],"source":["# Let's take that as our baseline accuracy.\n","# Let's see if we can do better by using K-Means as a preprocessing step.\n","# We will create a pipeline that will first cluster the training set into 50 clusters and replace\n","# the images with their distances to the 50 clusters, then apply a logistic regression model:\n","from sklearn.pipeline import Pipeline\n","\n","pipeline = Pipeline\n","\n","pipeline.fit(X_train, y_train)\n","\n","# This pipeline is equivalent to: \n","# kmeans = KMeans(n_clusters=50, random_state=seed)\n","# log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=seed)\n","# new_X_train = kmeans.fit_transform(X_train)\n","# log_reg.fit(new_X_train, y_train) \n","\n","# Thus `KMeans` is used to transform the training data. The original data, which has 64 features,\n","# is transformed into data with k features consisting of distances of the instances to the k centroids.\n","# This transformed data, together with the original labels, is then used to fit LogisticRegression.\n","# Prediction works the same way: the test data is first transformed by `KMeans` and then `LogisticRegression`\n","# is used with the transformed data to predict labels. Thus, instead of `pipeline.predict(X_test)` one\n","# could use `log_reg.predict(kmeans.transform(X_test))`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmboQQsi5FFk","outputId":"440ffdba-b61d-4985-d9e4-a786b86997ef"},"outputs":[],"source":["pipeline_score = pipeline.score(X_test, y_test)\n","pipeline_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECbO9o-l5FFk","outputId":"a90aadaf-e808-4ab0-caf4-019ac1c58318"},"outputs":[],"source":["X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eg5g4RoO5FFk"},"outputs":[],"source":["# We reduced the error by some percent.\n","# But we chose the number of clusters `k` completely arbitrarily, we can surely do better.\n","# Since K-Means is just a preprocessing step in a classification pipeline, finding a good\n","# value for `k` is much simpler now. The best value of `k` is simply the one that results\n","# in the best classification performance.\n","\n","\n","#### Warning: the following cell may take close to 10 minutes to run, or more depending on your hardware.\n","# from sklearn.model_selection import GridSearchCV\n","# param_grid = dict(kmeans__n_clusters=range(2, 100))\n","# grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2,  n_jobs=-1)\n","# grid_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28PygkKi5FFk"},"outputs":[],"source":["# Let's see what the best number of clusters is:\n","# grid_clf.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXhEmCZ25FFl"},"outputs":[],"source":["# grid_clf.score(X_test, y_test)\n","\n","# Cool! With k = 99 clusters, we get a significant accuracy boost on the test set. Though, you may\n","# want to keep exploring higher values for k, since 99 was the largest value in the range we explored."]},{"cell_type":"markdown","metadata":{"id":"cgIo6VsN5Oax"},"source":["# Scale data before clustering?\n","\n","It depends on your data.\n","\n","If you have attributes with a well-defined meaning, say, latitude and longitude, then you should not scale your data, because this will cause distortion. (K-means might be a bad choice too - you need something that can handle lat/lon naturally).\n","\n","If you have mixed numerical data, where each attribute is something entirely different (say, shoe size and weight), has different units attached (lb, tons, m, kg ...) then these values aren't really comparable anyway; z-standardizing them is the best-practise to give equal weight to them.\n","\n","If you have binary values, discrete attributes or categorial attributes, stay away from k-means. K-means needs to compute means, and the mean value is not meaningful on this kind of data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":2709,"status":"ok","timestamp":1679666230112,"user":{"displayName":"Alireza Fazeli","userId":"13019604121988176443"},"user_tz":240},"id":"1VI9CJ2S5VJp","outputId":"9d90680d-19c6-4dca-c76e-3ac4eba43030"},"outputs":[],"source":["rnorm = np.random.randn\n","\n","x = rnorm(1000) * 10  \n","y = np.concatenate([rnorm(500), rnorm(500) + 5])\n","\n","fig, axes = plt.subplots(3, 1)\n","\n","axes[0].scatter(x, y)\n","axes[0].set_title('Data (note different axes scales)')\n","\n","km = KMeans(2)\n","\n","clusters = km.fit_predict(np.array([x, y]).T)\n","\n","axes[1].scatter(x, y, c=clusters, cmap='bwr')\n","axes[1].set_title('non-normalised K-means')\n","\n","clusters = km.fit_predict(np.array([x / 10, y]).T)\n","\n","axes[2].scatter(x, y, c=clusters, cmap='bwr')\n","axes[2].set_title('Normalised K-means')"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
