{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import recall_score, confusion_matrix, roc_curve, auc, classification_report\n",
    "seed=738"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset describes grains of rice in terms of visual properties.\n",
    "# Class is rice variety: either Cammeo or Osmancık.\n",
    "# https://archive.ics.uci.edu/ml/datasets/Rice+%28Cammeo+and+Osmancik%29\n",
    "# (Actually downloaded the data from the paper's website.)\n",
    "rice = \n",
    "rice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out class distribution\n",
    "rice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline accuracy? (i.e. accuracy for always guessing the majority class)\n",
    "A = rice.               # majority class\n",
    "B = rice.\n",
    "\n",
    "baselineacc =           # the classifier you train must beat this number else no point in training one\n",
    "\n",
    "print('Baseline Accuracy: '+str((baselineacc*100).round(1))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, train logistic regression\n",
    "\n",
    "X = rice.\n",
    "y = rice.\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model, print coefficients\n",
    "ricelr = \n",
    "\n",
    "print(f\"Intercept:\\n {ricelr.intercept_.round(3)} \\nCoefficients:\\n {ricelr.coef_.round(3)}\")\n",
    "# This tells you how each one of the features relates to the probability that an example is from the positive or negative class  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict manually\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x)) # a small anonymous function that can take any number of args but can only have one expression\n",
    "z = ricelr.\n",
    "\n",
    "z.round(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can read more about Lambda [here](https://www.w3schools.com/python/python_lambda.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's see how probabilities look like:\n",
    "sigmoid(z).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can use a built-in function to see the probabilities without hard coding:\n",
    "ricelr.\n",
    "\n",
    "# first column: probabilities for class 0 (one minus second column),\n",
    "# second column: probabilities for class 1 (calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label predictions\n",
    "ytest_hat = \n",
    "\n",
    "ytest_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance measures from scratch\n",
    "# TP: true positives \n",
    "# TN: true negatives \n",
    "# FP: False positives \n",
    "# FN: False negatives\n",
    "\n",
    "def compute_performance(yhat, y, classes):\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat == classes[1], y == classes[1]))\n",
    "    tn = sum(np.logical_and(yhat == classes[0], y == classes[0]))\n",
    "    fp = sum(np.logical_and(yhat == classes[1], y == classes[0]))\n",
    "    fn = sum(np.logical_and(yhat == classes[0], y == classes[1]))\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    # Recall\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    recall = tp / (tp + fn)    \n",
    "    \n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = recall\n",
    "    \n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    # Print results\n",
    "    \n",
    "    print(\"Accuracy:\",round(acc,3),\"Recall:\",round(recall,3),\"Precision:\",round(precision,3),\n",
    "          \"Sensitivity:\",round(sensitivity,3),\"Specificity:\",round(specificity,3))\n",
    "\n",
    "compute_performance(ytest_hat, ytest, ricelr.classes_)\n",
    "\n",
    "# Let's compare against base-line accuracy:\n",
    "print('\\nBaseline Accuracy: ', baselineacc.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's experiment by adjusting the decision threshold\n",
    "threshold = 0.1\n",
    "ytest_prob = ricelr.predict_proba(Xtest)\n",
    "\n",
    "# ytest_prob\n",
    "\n",
    "# ytest_prob[:,1].round(3) > threshold\n",
    "\n",
    "# [(ytest_prob[:,1] > threshold).astype(int)] # turn them into 0 and 1\n",
    "\n",
    "# ricelr.classes_ # element 0 for label class 0 and element 1 for label class 1\n",
    "\n",
    "yhat = ricelr.\n",
    "\n",
    "# yhat\n",
    "\n",
    "compute_performance(yhat, ytest, ricelr.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking based criterion: Receiver Operating Characteristic (ROC) curve using sklearns:\n",
    "  \n",
    "fpr, tpr, _ =           # 2nd arg: ranking score, 3rd arg: \"Osmancik\"\n",
    "\n",
    "ax =sns.lineplot(x=fpr,y=tpr)\n",
    "ax.set_xlabel(\"FP Rate\")\n",
    "ax.set_ylabel(\"TP Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under ROC (AUROC)\n",
    "auc()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "iris = pd.read_csv(\"Dataset_iris.csv\")\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out class distribution\n",
    "iris.\n",
    "\n",
    "# we have 3 balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create y and X. Not going to split these data for this demonstration.\n",
    "X = iris.drop(\"Species\", axis=\"columns\")\n",
    "y = iris.Species.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that unlike our convention in class, sklearn makes a parameter vector\n",
    "# for every class (not just first K-1) even though it is redundant.\n",
    "IRISLR = LogisticRegression(penalty=None)\n",
    "irislr = IRISLR.fit(X,y)\n",
    "print(f\"Intercepts:\\n {irislr.intercept_.round(3)} \\n\\nCoefficients:\\n {irislr.coef_.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = irislr.\n",
    "# yhat\n",
    "yhat_probs = irislr.\n",
    "# yhat_probs.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix\n",
    "\n",
    "# We transpose to get it in the format we saw in slides:\n",
    "# Rows: Predicted labels\n",
    "# Columns: True labels "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2: sum of the squares of the values of the coefficients\n",
    "# C: smaller values specify stronger regularization\n",
    "IRISLR = LogisticRegression                                # Experiment different values for C\n",
    "\n",
    "irislr = IRISLR.fit(X,y)\n",
    "print(f\"Intercepts:\\n {irislr.intercept_.round(3)} \\n\\nCoefficients:\\n {irislr.coef_.round(3)}\")\n",
    "\n",
    "yhat = irislr.predict(X)\n",
    "yhat_probs = irislr.predict_proba(X)\n",
    "confusion_matrix(yhat,y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could also use `classification_report` from sklearn to output different evaluation metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the MNIST dataset, which is a set of 70000 small images of handwritten digits.\n",
    "# Each image is labeled with the digit it represents.\n",
    "\n",
    "from sklearn.datasets import fetch_openml # a helper function to download popular datasets\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets loaded by Scikit-Learn generally have a similar dictionary structure, including the following:\n",
    "\n",
    "print(mnist.keys())\n",
    "# print('\\n', mnist['DESCR']) # DESCR key: describing the dataset\n",
    "# print('\\n', mnist['data']) # data key: contains an array with one row per instance and one column per feature\n",
    "print('\\n', mnist['target']) # target key: contains an array with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s look at X and y:\n",
    "X, y = \n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 70000 images, and each image has 784 features. This is because each image\n",
    "# is 28 × 28 pixels, and each feature simply represents one pixel’s intensity, from 0\n",
    "# (white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to\n",
    "# do is grab an instance’s feature vector, reshape it to a 28 × 28 array, and display it\n",
    "# using Matplotlib’s imshow() function. Let's write a function for this task:\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks like a 5, and indeed that’s what the label tells us:\n",
    "print('Its label is '+y[0]+', with type being',type(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the label is a string. Most ML algorithms expect numbers, so let’s cast y to integer:\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To give you a feel for the complexity of the classification task, let's see more images from the dataset.\n",
    "# Let's create a function first:\n",
    " \n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    # This is equivalent to n_rows = ceil(len(instances) / images_per_row):\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Append empty images to fill the end of the grid, if needed:\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n",
    "\n",
    "    # Reshape the array so it's organized as a grid containing 28×28 images:\n",
    "    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n",
    "\n",
    "    # Combine axes 0 and 2 (vertical image grid axis, and vertical image axis),\n",
    "    # and axes 1 and 3 (horizontal axes). We first need to move the axes that we\n",
    "    # want to combine next to each other, using transpose(), and only then we\n",
    "    # can reshape:\n",
    "    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n",
    "                                                         images_per_row * size)\n",
    "    # Now that we have a big image, we just need to show it:\n",
    "    plt.imshow(big_image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the function to see more images:\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see a few labels and compares with the image above:\n",
    "y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set and set it aside:\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s simplify the problem for now and only try to identify one digit, for example, the number 5. \n",
    "# This “5-detector” will be an example of a binary classifier, capable of distinguishing between\n",
    "# just two classes, 5 and not-5.\n",
    "\n",
    "# Create new y: True for all 5s, False for all other digits:\n",
    "y_train_5 = \n",
    "y_test_5 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let’s start with a Stochastic Gradient Descent classifier, using Scikit-Learn’s SGDClassifier class.\n",
    "# This classifier has the advantage of being capable of handling very large datasets efficiently.\n",
    "# Create an SGDClassifier and train it on the whole training set:\n",
    "\n",
    "sgd_clf = \n",
    "\n",
    "sgd_clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use it to detect images of the number 5:\n",
    "some_digit = \n",
    "sgd_clf.\n",
    "# Looks like it guessed it right. Because if you look at\n",
    "# the image above the first element is indeed a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s look at the baseline score:\n",
    "\n",
    "A =  # the majority class\n",
    "B = \n",
    "\n",
    "baselineacc = A/(A+B)\n",
    "print('Baseline Accuracy:', baselineacc.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That’s right, the baseline accuracy is 91%! \n",
    "# This is simply because only about 9% of the images are 5s. So if you always\n",
    "# guess that an image is not a 5, you will be right about 91% of the time\n",
    "\n",
    "# Percentage of images that are 5s:\n",
    "(B/(A+B)*100).round(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c83c3ca59419a9bd6335317d30070f5f03c1d7834fcd61b98e82c2edec68ebb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
