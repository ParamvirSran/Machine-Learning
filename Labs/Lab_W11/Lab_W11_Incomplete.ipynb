{"cells":[{"cell_type":"markdown","metadata":{"id":"L0ukRfuauz6I"},"source":["# Dimensionality Reduction\n","\n","In this lab, we will work with an IMDB dataset to estimate the sentiment of movie reviews. We will study PCA and Sparse PCA in this context, and work using Single Value Decomposition to perform topic analysis. In the context of text mining, we call SVD *Latent Semantic Analysis* (LSA).\n","\n","LSA is already implemented in Python in scikit-learn in the package [*TruncatedSVD*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), we will use that along with the Natural Language Processing library [*NLTK*](https://www.nltk.org/) for our methods.\n","\n","The general process can be summarized as follows:\n","\n","1. Load the text in free form.\n","2. Preprocess the text to normalize it.\n","3. Calculate LSA.\n","4. Explore the results."]},{"cell_type":"markdown","metadata":{"id":"eTsCGrpbuz6N"},"source":["## Loading text: IMDB database.\n","\n","This dataset comes from the website Internet Movie Database, and represents 25,000 reviews which were labeled (by humans) as positive or negative, see [here](http://ai.stanford.edu/~amaas/data/sentiment/) for more details. It is a pretty big dataset, so we will work with small samples of 500 positive cases and 500 negative cases.\n","\n","The uncompressed data is simply a series of text documents, each in its own text file, stored in two classes (Positive and Negative), one per folder:\n","\n","The first step is to load the data and create a \"corpus\". A corpus is, quite simply, a set of documents. Here, we will read the files from our folders, and assign it a sentiment. We need to read the documents one by one, and store them into a data object which will have the texts and a tag highlighting whether they are positive or negative."]},{"cell_type":"markdown","metadata":{"id":"COP_KBcSuz6N"},"source":["### Reading the text\n","\n","The first step is to read the data into a vector. We need to read from the document path, using the internal system. This package is called `os` and comes pre-installed in Python.\n"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":[],"metadata":{"ExecuteTime":{"start_time":"2024-03-20T11:13:13.697365Z","end_time":"2024-03-20T11:13:14.101776Z"},"id":"n3pNePNtMqzI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2A7SXKW0T4s","ExecuteTime":{"start_time":"2024-03-20T11:13:13.716301Z","end_time":"2024-03-20T11:13:14.309146Z"}},"outputs":[],"source":["# imports\n","import os\n","import numpy as np; seed = 2316; np.random.seed(seed)\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","from   scipy.sparse import csr_matrix\n","import sklearn.feature_extraction.text as sktext\n","from   sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n","\n","# https://scikit-learn.org/stable/modules/manifold.html\n","from   sklearn.manifold import TSNE\n","\n","\n","\n","import umap.plot # good for plotting gigantic data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6Rv_wR9v_R_","ExecuteTime":{"start_time":"2024-03-20T13:53:08.410196Z","end_time":"2024-03-20T13:53:08.433123Z"}},"outputs":[],"source":["import umap.umap_ as umap"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":[],"metadata":{"ExecuteTime":{"start_time":"2024-03-20T13:52:37.024032Z","end_time":"2024-03-20T13:52:37.042979Z"},"id":"smTbTyTtMqzK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn54TuS5uz6O","ExecuteTime":{"start_time":"2024-03-20T11:13:13.747200Z","end_time":"2024-03-20T11:13:14.351006Z"}},"outputs":[],"source":["# List all files in the positive samples. Replace with your own!\n","dir = 'C:/Users/jojojoe/Documents/DS 3000_9000 TA/Week_11/Week_11/LSA_Sample/Lecture_Sample/train/pos/'\n","fileList = os.listdir(dir)\n","\n","# Create vector with texts\n","outtexts = []\n","\n","# Read the files in the directory and append them with the class to the dataset\n","for eachFile in fileList:\n","    with open(dir + eachFile, 'rb', newline = None) as _fp:\n","        fileData = _fp.read()\n","        outtexts.append(fileData)\n","    _fp.close()\n","\n","# Create dataframe from outputs\n","texts = pd.DataFrame({'texts': outtexts, 'class': 1})\n","print(texts.shape)\n","texts.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYyiR9Y0uz6Q","ExecuteTime":{"start_time":"2024-03-20T11:13:13.812018Z","end_time":"2024-03-20T11:13:14.394369Z"}},"outputs":[],"source":["# Repeat for negative values\n","# List all files in the \"pos\" directory\n","dir = 'C:/Users/jojojoe/Documents/DS 3000_9000 TA/Week_11/Week_11/LSA_Sample/Lecture_Sample/train/neg/'\n","fileList = os.listdir(dir)\n","\n","# Create vector with texts\n","outtexts = []\n","\n","# Read the files in the directory and append them with the class to the dataset\n","for eachFile in fileList:\n","    with open(dir + eachFile, 'rb', newline = None) as _fp:\n","        fileData = _fp.read()\n","        outtexts.append(fileData)\n","    _fp.close()\n","\n","# Create dataframe from outputs\n","texts = pd.concat((texts, pd.DataFrame({'texts': outtexts, 'class': 0})), ignore_index = True)\n","texts.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NsBTAHRMuz6Q","ExecuteTime":{"start_time":"2024-03-20T11:13:13.888323Z","end_time":"2024-03-20T11:13:14.398356Z"}},"outputs":[],"source":["texts.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMKZins2wN_P","ExecuteTime":{"start_time":"2024-03-20T13:54:07.129960Z","end_time":"2024-03-20T13:54:07.262116Z"}},"outputs":[],"source":["##See how the text looks like"]},{"cell_type":"markdown","metadata":{"id":"f_f8vrDh0T4y"},"source":["The text is quite dirty, so we'll use regex code to clean it. It is available in Python using the package [re](https://www.rexegg.com/regex-quickstart.html). Regex can be daunting, but it is very rewarding to learn. Do spend some time with it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPuCeQen0T4z","ExecuteTime":{"start_time":"2024-03-20T11:13:13.947431Z","end_time":"2024-03-20T11:13:14.422277Z"}},"outputs":[],"source":["CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n","def cleanhtml(raw_html):\n","    html = raw_html.decode('ISO-8859-1') # Change the encoding to your locale!\n","    cleantext = re.sub(CLEANR, '', html)\n","    return cleantext\n","\n","texts['texts'] = texts['texts'].apply(cleanhtml)\n","texts"]},{"cell_type":"markdown","metadata":{"id":"AKv29r8H0T4z"},"source":["Now we will transform the text. The following code uses sklearn's [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) which applies a [Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) transformation to the text ( which means counting how many times a certain concept, *e.g.*, a word, appears in the document versus the total times it appears in the document) to do the following:\n","\n","1. Eliminate accents and other characters.\n","2. Eliminate the so-called \"stopwords\", or words that are irrelevant to the learning given they are only connectors. These words are [here](https://gist.github.com/ethen8181/d57e762f81aa643744c2ffba5688d33a).\n","3. Eliminate concepts that are rare (min_df) or too common (max_df). Here we eliminate concepts that appear in less than 5% of documents and those that appear in over 90%.\n","\n","The last argument calculates a logaritmic (or sublinear) transformation, which is more robust. This effectively transforms our text data into a fully numeric dataset!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lT9LAUx0uz6R","ExecuteTime":{"start_time":"2024-03-20T11:13:13.978662Z","end_time":"2024-03-20T11:13:14.437226Z"}},"outputs":[],"source":["# Transform the text\n","TfIDFTransformer = sktext.TfidfVectorizer(strip_accents=, # Eliminate accents and special characters\n","                      stop_words=, # Eliminates stop words.\n","                      , # Eliminate words that do not appear in more than 5% of texts\n","                      , # Eliminate words that appear in more than 95% of texts\n","                      # Use sublinear weights (softplus), i.e., replace tf with 1 + log(tf)\n","                      )"]},{"cell_type":"markdown","metadata":{"id":"JwCWLFfBuz6R"},"source":["The model structure of scikit-learn follows always the same:\n","\n","1. We define the model using the appropriate function directly from the package (as above).\n","\n","2. We train the model using the \"fit\" method over the object we created in 1.\n","\n","3. We apply the model to data using the \"transform\" method.\n","\n","In cases where we want to fit *and* transform the inputs - such as a TF-IDF transform, which is applied over the same data where the weights are \"trained\" - we can use directly the method \"fit_transform\", that performs steps 2 and 3 directly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vq6XbDpMuz6S","ExecuteTime":{"start_time":"2024-03-20T11:13:13.993612Z","end_time":"2024-03-20T11:13:14.460149Z"}},"outputs":[],"source":["TfIDF_IMDB = TfIDFTransformer.\n","TfIDF_IMDB"]},{"cell_type":"markdown","metadata":{"id":"Wgi8xY6-5Puj"},"source":["It's sparse because not every word appears in every document, leading to null entries. These matrices only store the relevant information! They are much more efficient in memory use and compute time. Unlike operations with dense matrices, operations with sparse matrices do not perform unnecessary low-level arithmetic, such as zero-adds (x+0 is always x). The resulting efficiencies can lead to dramatic improvements in execution time for programs working with large amounts of sparse data. For a full matrix, you store 8 bytes (one double) per entry. For a sparse matrix, you store 12 bytes per entry (one double for the value, and one integer for the column index of the entry).\n","\n","Here, we have $23848$ sparse entries: $23848 \\times 12=286176$ bytes, and in the case of dense we have $1000 \\times 230 \\times 8 = 1840000 $ bytes. Since $ 286176 < 1840000 $, using sparse should yield better computational efficiency. Use sparse as long as $ 23848 \\times 12 < 1000 \\times 230 \\times 8 $ (or, $ 23848<67\\% (1000 \\times 230)$).\n","\n"," `TfIdfVectorizer.fit_transform()` has a `toarray()` method if needed.\n","\n","FYI: If you have a matrix with a lot of null entries, you could convert it to a sparse matrix using `from scipy.sparse import csr_matrix`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TICZOSEC5Puj","ExecuteTime":{"start_time":"2024-03-20T11:13:14.121710Z","end_time":"2024-03-20T11:13:14.460149Z"}},"outputs":[],"source":["data_size = # or 1000*230*8/(1024**2)\n","print('Size of dense matrix: '+ '%3.2f' %data_size + ' MB')\n","\n","data_csr_size=\n","print('Size of sparse csr_matrix: '+ '%3.2f' %data_csr_size + ' MB') # or 23848*12/(1024**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSNm_2c50T42","ExecuteTime":{"start_time":"2024-03-20T11:13:14.134665Z","end_time":"2024-03-20T11:13:14.461146Z"}},"outputs":[],"source":["df1 = pd.DataFrame(TfIDF_IMDB.toarray(), columns=TfIDFTransformer.get_feature_names_out())\n","df1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwssZpiN0T42","ExecuteTime":{"start_time":"2024-03-20T11:13:14.168072Z","end_time":"2024-03-20T11:13:14.461146Z"}},"outputs":[],"source":["vectorizer = sktext.CountVectorizer(, # Eliminate words that appear in more than 95% of texts\n","                      )\n","matrix =\n","df2 = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","print(df2.shape)\n","df2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Or1I45-v0T42","ExecuteTime":{"start_time":"2024-03-20T11:13:14.310137Z","end_time":"2024-03-20T11:13:14.461146Z"}},"outputs":[],"source":["# Show us the top 10 most common words\n","df2\n","# df2.sum().sum()"]},{"cell_type":"markdown","metadata":{"id":"rP3WH65M0T43"},"source":["The output of the TF-IDF transformer is a sparse matrix. We can check the outputs of the first row with the below code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnj4CNPBuz6S","ExecuteTime":{"start_time":"2024-03-20T11:13:14.324090Z","end_time":"2024-03-20T11:13:14.665838Z"}},"outputs":[],"source":["print(TfIDF_IMDB[0,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c4MyJLx0T43","ExecuteTime":{"start_time":"2024-03-20T11:13:14.340039Z","end_time":"2024-03-20T11:13:14.696575Z"}},"outputs":[],"source":["# and we can verify:\n","df1.iloc[0,173]"]},{"cell_type":"markdown","metadata":{"id":"lB06CGntuz6T"},"source":[" The following vector shows the list of words associated to each index for indices 30 to 39."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDAXoohsuz6T","ExecuteTime":{"start_time":"2024-03-20T11:13:14.355989Z","end_time":"2024-03-20T11:13:14.697630Z"}},"outputs":[],"source":["# Let's save the indexes for later.\n","word_index =\n","print(word_index[30:40])"]},{"cell_type":"markdown","metadata":{"id":"ZexSmkGZ0T44","tags":[]},"source":["## [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [Sparse PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html)\n","\n","You have an $n\\times p$ data matrix $\\boldsymbol X$, where $n$ is the number of samples and $p$ is the number of features. The singular value decomposition (SVD) of $\\boldsymbol X$ gives three matrices $\\boldsymbol{U\\Sigma V}^*$. Combining the first two ($\\boldsymbol {Z=U\\Sigma}$) gives the matrix of principal components. $\\boldsymbol {\\Sigma }$ is a diagonal $p\\times p$ matrix containing the singular values in descending order. $\\boldsymbol {U}$ and $\\boldsymbol {V}^*$ are unitary/orthogonal matrices (*i.e.*, rotation matrices).\n","\n","$\\boldsymbol V$ contains the principal loading vectors. This means that the principal components are derived by using the principal loadings as coefficients in a linear combination of your data matrix.\n","\n","Once you have all the PCs (*i.e.*, $\\boldsymbol {U\\Sigma }$) figured out you can use the eigenvalues (the sum of squares of the distances from the origin after the projection of the datapoints to the PC, in other words, the square of singular values) to determine the proportion of variation that each PC accounts for."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s6s13yY0T44","ExecuteTime":{"start_time":"2024-03-20T11:13:14.374436Z","end_time":"2024-03-20T11:13:14.713193Z"}},"outputs":[],"source":["# Do normal PCA on the data set\n","n = 2\n","nPCA =\n","\n","# Now we fit. We need to transform our matrix to dense format first.\n","nPCA.fit(TfIDF_IMDB.toarray())\n","\n","# Let's calculate the variance of the two components. (lambda_i over sum of lambdas gives amount of explained variance)\n","total_variance=\n","print('Total variance explained by the first %i components is %.3f.' % (nPCA.n_components_, total_variance))\n","\n","total_variance_ratio =\n","print('The first %i components explain %.3f%% of total variance.' % (nPCA.n_components_, total_variance_ratio))\n","\n","# Let's get the components and plot them, coloring by the class\n","Z1 =\n","sns.scatterplot(x=Z1[:, 0], y=Z1[:, 1], hue=texts['class'])\n","\n","plt.xlabel(\"Component 1\")\n","plt.ylabel(\"Component 2\")\n","plt.title(\"PCA of IMDB texts\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q6ZLFKF0T46","ExecuteTime":{"start_time":"2024-03-20T11:13:14.670982Z","end_time":"2024-03-20T11:13:14.853214Z"}},"outputs":[],"source":["### SVD with numpy\n","\n","X = TfIDF_IMDB.toarray()\n","X = X - X.mean(axis=0)\n","print(\"Shape of X:\", X.shape)\n","\n","u, s, vt = np.linalg.svd(X, full_matrices=False, compute_uv=True) # It's not necessary to compute the full matrix of U or V\n","\n","# flip eigenvectors' sign to enforce deterministic output\n","def svd_flip(u, v):\n","        max_abs_cols = np.argmax(np.abs(u), axis=0)\n","        signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n","        u *= signs\n","        v *= signs[:, np.newaxis]\n","        return u, v\n","\n","u, vt = svd_flip(u, vt)\n","\n","# The columns of u are the principal components scaled to unit norm.\n","print(\"\\nShape of U:\", u.shape)\n","\n","print(\"Shape of S:\", s.shape)\n","\n","# The columns of v contain the principal axes.\n","print(\"Shape of V.T:\", vt.shape)\n","\n","print(\"\\nCan X be reconstructed using U, S, and V.T?\\n{}\".format(np.allclose(X, np.dot(u * s, vt))))\n","\n","print(\"\\nFirst {} singular values calculated manually:\\n{}\".format(n, s[:n]))\n","\n","# get the singular values from sklearn\n","print(\"\\nFirst {} singular values calculated by sklearn:\\n{}\".format(n, nPCA.singular_values_))"]},{"cell_type":"markdown","metadata":{"id":"t9YRgu0R5Puk"},"source":["To a obtain a reduced dataset $\\boldsymbol X_{d}$, compute the matrix multiplication of the training set matrix $\\boldsymbol X$ by the matrix containing the first $d$ eigenvectors (columns) of $\\boldsymbol V$.\n","\n","$\\boldsymbol V$: Principal axes in feature space, representing the directions of maximum variance in the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1S7hQy_M5Puk","ExecuteTime":{"start_time":"2024-03-20T11:13:14.761997Z","end_time":"2024-03-20T11:13:14.884630Z"}},"outputs":[],"source":["print(X.shape)\n","\n","d=2\n","(X@vt.T[:,:d]).shape # this gives a reduced dataset which is 2 dimensional"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtJ_nMj95Puk","ExecuteTime":{"start_time":"2024-03-20T11:13:14.777455Z","end_time":"2024-03-20T11:13:14.885623Z"}},"outputs":[],"source":["print(vt.shape)\n","vt[0,:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlNOtVCS5Puk","ExecuteTime":{"start_time":"2024-03-20T11:13:14.792410Z","end_time":"2024-03-20T11:13:14.886620Z"}},"outputs":[],"source":["print(nPCA.components_.shape) # of shape (n_components, n_features)\n","nPCA.components_[0,:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK4vUTbF0T46","ExecuteTime":{"start_time":"2024-03-20T11:13:14.808354Z","end_time":"2024-03-20T11:13:14.896591Z"}},"outputs":[],"source":["loadings = pd.DataFrame(nPCA.components_.T, columns=['PC1', 'PC2'], index=word_index)\n","loadings\n","# Principal axes in feature space, representing the directions of maximum variance in the data.\n","# Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors.\n","\n","# The columns of the dataframe contain the eigenvectors associated\n","# with the first two principal components. Each element represents\n","# a loading, namely how much (the weight) each original variable\n","# contributes to the corresponding principal component."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXN-NiS80T46","ExecuteTime":{"start_time":"2024-03-20T11:13:14.841243Z","end_time":"2024-03-20T11:13:14.896591Z"}},"outputs":[],"source":["# Get explained variance from class attribute:\n","# The variance of the training samples transformed by a projection to each component (i.e. the eigenvalues of the covariance matrix)\n","nPCA.explained_variance_ # contains the diagonal elements of the covariance of the two principal components"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kb4uC8_Y5Pul","ExecuteTime":{"start_time":"2024-03-20T11:13:14.857198Z","end_time":"2024-03-20T11:13:14.961371Z"}},"outputs":[],"source":["# Calculate explained variance manually (from the covariance matrix):\n","explained_variance=np.cov(Z1.T).diagonal()\n","explained_variance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAdnj_FJ0T47","ExecuteTime":{"start_time":"2024-03-20T11:13:14.871670Z","end_time":"2024-03-20T11:13:14.985469Z"}},"outputs":[],"source":["# Get percentage of variance explained by each of the selected components from class attribute:\n","print(\"Explained variance ratio:\", nPCA.explained_variance_ratio_)\n","\n","print(\"Sum of explained variance ratio:\", sum(nPCA.explained_variance_ratio_))\n","# The sum does not add up to 1 implying that the rest of\n","# the variances are contained in the other components."]},{"cell_type":"markdown","metadata":{"id":"otIbUL7q0T47"},"source":["Explained Variance Ratio = $\\frac{Explained Variance}{\\sum Explained Variance}$\n","\n","To hard code the above formula, must make sure to include in the denominator all variances and not only the variance explained by the components that you have shrunken into. And for that, need to calculate the trace of the covariance of the original X (i.e. features matrix) before PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phUQov0u0T48","ExecuteTime":{"start_time":"2024-03-20T11:13:14.887616Z","end_time":"2024-03-20T11:13:14.986469Z"}},"outputs":[],"source":["# Calculate percentage of variance explained by each of the selected components manually:\n","\n","# total_variance=np.sum(np.cov(TfIDF_IMDB.toarray().T).diagonal())\n","# or\n","total_variance=np.sum(np.trace(np.cov(TfIDF_IMDB.toarray().T)))\n","\n","explained_variance_ratio=explained_variance/total_variance\n","\n","print(\"Sum of explained variance ratio:\", sum(explained_variance_ratio))\n","# The sum does not add up to 1 implying that the rest of\n","# the variances are contained in the other components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-u72sdR0T48","ExecuteTime":{"start_time":"2024-03-20T11:13:14.902566Z","end_time":"2024-03-20T11:13:14.986469Z"}},"outputs":[],"source":["nPCA.components_.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsxxxPrR5Pux","ExecuteTime":{"start_time":"2024-03-20T11:13:14.919509Z","end_time":"2024-03-20T11:13:14.986469Z"}},"outputs":[],"source":["len(word_index[nPCA.components_[0,:] != 0]) # all 230 features showed up on the 1st PC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mYFXDBa0T49","ExecuteTime":{"start_time":"2024-03-20T11:13:14.934459Z","end_time":"2024-03-20T11:13:14.987616Z"}},"outputs":[],"source":["# Words that are positively correlated to the first component\n","word_index[nPCA.components_[0,:] > 0] # < 0 gives words with negative correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxL2bTR20T49","ExecuteTime":{"start_time":"2024-03-20T11:13:14.949411Z","end_time":"2024-03-20T11:13:14.987616Z"}},"outputs":[],"source":["# Words that are positively correlated to the second component\n","word_index[nPCA.components_[1,:] > 0]"]},{"cell_type":"markdown","metadata":{"id":"F4rzqdmR0T49"},"source":["Even though we only explain 3.4% of the variance in the data we still see some separation!"]},{"cell_type":"markdown","metadata":{"id":"dTFzIl_V0T49"},"source":["Regular PCA has a major shortcoming: each principal component is a linear combination of all the original features, and their coefficients are typically non-zero. This can make interpretation hard, especially when a certain number of principal components is chosen.\n","\n","Sparse PCA finds the set of sparse components that can optimally reconstruct the data. The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter `alpha`.\n","\n","Sparsity is enforced most of the time on the principal loadings, _i.e.,_ entries of $\\boldsymbol V$. The advantage of sparsity is that a sparse $\\boldsymbol V$ tells us which variables from the original $p$-dimensional feature space are worth keeping. This helps with interpretability.\n","\n","Spareness can also be imposed on $\\boldsymbol {Z}$. This will make each observation loading appear on\n","few principal vectors. This is less popular.\n","\n","Let's see if [Sparse PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html) does it better:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IC2ySJb70T4-","ExecuteTime":{"start_time":"2024-03-20T11:13:14.963920Z","end_time":"2024-03-20T11:13:17.587064Z"}},"outputs":[],"source":["# Now do sparse PCA enforcing sparseness on variables\n","# that means only a few of the original variables can appear on each latent factor\n","sPCA =  # play with alpha to control sparsity. Higher values lead to sparser components.\n","sPCA.fit(TfIDF_IMDB.toarray())\n","\n","# Get the results\n","Z2 =\n","\n","# Create plot\n","sns.scatterplot(x=Z2[:, 0], y=Z2[:, 1], hue=texts['class'])\n","plt.xlabel(\"Component 1\")\n","plt.ylabel(\"Component 2\")\n","plt.title(\"Sparse PCA of IMDB texts\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2P0IkJxL0T4-"},"source":["What happened? Given that we are forcing only some of the 230 components to be a part of the solution, this actually hurts the ability to get the components. Still, it allows to study if some components are more relevant! Now you would check which ones are actually present, like so."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bAlpyy20T4-","ExecuteTime":{"start_time":"2024-03-20T11:13:17.558646Z","end_time":"2024-03-20T11:13:17.602010Z"}},"outputs":[],"source":["print(sPCA.components_.shape)\n","len(word_index[sPCA.components_[0,:] != 0]) # only 60 features (out of 230) showed up on the 1st PC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47ltB_X60T4-","ExecuteTime":{"start_time":"2024-03-20T11:13:17.574108Z","end_time":"2024-03-20T11:13:17.602010Z"}},"outputs":[],"source":["# Words that are positively correlated to the first component\n","word_index[sPCA.components_[0,:] > 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqxwncvY0T4-","ExecuteTime":{"start_time":"2024-03-20T11:13:17.590049Z","end_time":"2024-03-20T11:13:17.723840Z"}},"outputs":[],"source":["# Words that are positively correlated to the second component\n","word_index[sPCA.components_[1,:] > 0]"]},{"cell_type":"markdown","metadata":{"id":"vMAWV-aN0T4-"},"source":["As we can see, the first two components are simply referring to the same thing. People that discusses movies and films. Go back and play around with the parameters and see whether you can improve these results.\n","\n","To get meaningful answers, we need to use truncated Singular Value Decomposition."]},{"cell_type":"markdown","metadata":{"id":"VQ4GJ708uz6U"},"source":["## Running [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) Decomposition\n","\n","\n","This estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently. In particular, truncated SVD works effectively on term count/tf-idf matrices. In that context, it is known as latent semantic analysis (LSA).\n","Its advantage over PCA is that it works out of the box on sparse data.\n"]},{"cell_type":"markdown","metadata":{"id":"_CDr9rgIuz6U"},"source":["Again, we first define it using the TruncatedSVD function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09BLfsQzuz6U","ExecuteTime":{"start_time":"2024-03-20T11:13:17.605997Z","end_time":"2024-03-20T11:13:17.723840Z"}},"outputs":[],"source":["svd = TruncatedSVD( # Random state. As SVD is rotation-invariant, we need to set this\n","                  )"]},{"cell_type":"markdown","metadata":{"id":"cJWKrH8yuz6V"},"source":["As we will apply the model to new data too, we need to first fit the model, not fit and transform simultaneously."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFjkl9uIuz6V","scrolled":true,"ExecuteTime":{"start_time":"2024-03-20T11:13:17.621943Z","end_time":"2024-03-20T11:13:17.757120Z"}},"outputs":[],"source":["# Train the model!\n","svd.fit(TfIDF_IMDB)"]},{"cell_type":"markdown","metadata":{"id":"3zDCChkWuz6V"},"source":["Now we can dig deeper into the model outputs. First, we calculate the explained variance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHlXUyqRuz6V","ExecuteTime":{"start_time":"2024-03-20T11:13:17.686255Z","end_time":"2024-03-20T11:13:17.758118Z"}},"outputs":[],"source":["# Get total explained variance in percentage\n","print('The components explain %.1f%% of total variance.' % (svd.explained_variance_ratio_.sum()*100))\n","# svd.explained_variance_ratio_"]},{"cell_type":"markdown","metadata":{"id":"ki2-kuo7uz6V"},"source":["The explained variance is approximately $68\\%$ of the total variance using 100 components.\n","\n","Let's study the relationship between components and words. The matrix *components*, inside our svd object, contains the **principal component matrix**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"so55yUMSuz6V","ExecuteTime":{"start_time":"2024-03-20T11:13:17.701549Z","end_time":"2024-03-20T11:13:17.759114Z"}},"outputs":[],"source":["# Get component x words matrix\n","svd.components_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Wz3a06kuz6W","ExecuteTime":{"start_time":"2024-03-20T11:13:17.718695Z","end_time":"2024-03-20T11:13:17.759114Z"}},"outputs":[],"source":["svd.components_.shape"]},{"cell_type":"markdown","metadata":{"id":"uCM3b2Eeuz6W"},"source":["We can focus on particular words or concepts too. For example, for the word \"action\" (in place 3) we can get the following weight vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05QH5qHJuz6W","ExecuteTime":{"start_time":"2024-03-20T11:13:17.737322Z","end_time":"2024-03-20T11:13:17.868869Z"}},"outputs":[],"source":["word_index[3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY46Td28uz6X","ExecuteTime":{"start_time":"2024-03-20T11:13:17.749809Z","end_time":"2024-03-20T11:13:17.870043Z"}},"outputs":[],"source":["svd.components_[:, 3]"]},{"cell_type":"markdown","metadata":{"id":"NvVu4zsRuz6X"},"source":["This means that the word \"action\" is positively related to concept 1, barely negatively related to concept 2 (i.e. that concept does *not* relate to \"action\"), etc.\n","\n","To get the five words that relate the most with concept 2, we need to reorder and sort the vector. This can be tricky, so we will use Numpy's *argpartition* function, which will give us the **unsorted** top X values. See the discussion [here](https://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOiVEmi7uz6X","ExecuteTime":{"start_time":"2024-03-20T11:13:17.766090Z","end_time":"2024-03-20T11:13:17.870043Z"}},"outputs":[],"source":["# To get the second concept, remember that Python starts indexing from 0.\n","indices = np.argpartition(svd.components_[1, :], -10)[-10:]\n","indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Byk3zW1g0T5D","ExecuteTime":{"start_time":"2024-03-20T11:13:17.779405Z","end_time":"2024-03-20T11:13:17.870043Z"}},"outputs":[],"source":["# Get the words.\n","word_index[indices]"]},{"cell_type":"markdown","metadata":{"id":"gLu3_UKxuz6X"},"source":["As we can see, the second concept appears to relate to \"bad movies\". Try concept 1; it will relate to \"popular movies\".\n","\n","The code below displays the singular values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYCKzdgiuz6Y","scrolled":true,"ExecuteTime":{"start_time":"2024-03-20T11:13:17.795885Z","end_time":"2024-03-20T11:13:17.871043Z"}},"outputs":[],"source":["# Get singular values\n","svd.singular_values_"]},{"cell_type":"markdown","metadata":{"id":"KRrUiPJ9uz6a"},"source":["Now we have a much better way to study these concepts! Of course, now we can study particular concepts or particular words, as desired.\n","\n","More importantly, we can now train a model over our reduced space, by using directly the components data matrix as our dataset!"]},{"cell_type":"markdown","metadata":{"id":"4X0D4PMi0T5D"},"source":["## Non-Linear Visualization\n","\n","Can we improve this analysis using non-linear methods? We will now study the use of t-Distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP) over the data. We will use two implementations:\n","\n","1. t-SNE is available in sklearn, in the [sklearn.manifold](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.manifold) subpackage.\n","2. The more efficient UMAP is available in its own package, called [umap](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)."]},{"cell_type":"markdown","metadata":{"id":"vMqeR2DH0T5E"},"source":["### t-SNE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WO6rGYv40T5E","ExecuteTime":{"start_time":"2024-03-20T13:37:34.595498Z","end_time":"2024-03-20T13:37:34.620478Z"}},"outputs":[],"source":["tSNEmapper = TSNE(n_components=2,               # How many dimensions to use. Never more than 2 or 3\n","                  init='random',                # First initialization. Sparse matrices need 'random'.  Otherwise use 'pca'\n","                  perplexity=50.0,              # Read below\n","                  early_exaggeration=12.0,      # Read below\n","                  learning_rate='auto',         # Related to above. Leave to auto\n","                  n_iter=5000,                  # Very important to let iterate enough\n","                  n_iter_without_progress=300,  # Set early stopping\n","                  metric='euclidean',           # Metric to use to calculate distances.\n","                  min_grad_norm=1e-7,           # Minimum gradient to continue iterating\n","                  verbose=0,                    # Verbosity\n","                  random_state=seed,            # Random seed\n","                  n_jobs=-1,                    # Parallel processes\n","                 )"]},{"cell_type":"markdown","metadata":{"id":"JWhUinhQ0T5E"},"source":["Some parameters in TSNE are extremely important. [This](https://distill.pub/2016/misread-tsne/) is a great paper going into detail in this regard.\n","\n","In particular:\n","\n","- Perplexity: The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results. In general, set to a high value and test a few.\n","- n_iter: The method must converge to be good. Set high and let the min_grad_norm and n_iter_without_progress stop the training.\n","- Metric: How to measure distances. Can be any keyword [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html). For text, cosine similarity is [known to work](https://academic.oup.com/bioinformatics/article/22/18/2298/318080).\n","\n","The other parameters are not as significant. Let's train the model!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y73T-7jT0T5E"},"outputs":[],"source":["TSNE_embedding ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJyfw8JJ0T5F"},"outputs":[],"source":["# Create plot\n","sns.scatterplot(x=TSNE_embedding[:, 0], y=TSNE_embedding[:, 1], hue=texts['class'])\n","plt.xlabel(\"Component 1\")\n","plt.ylabel(\"Component 2\")\n","plt.title(\"t-SNE projection of IMDB texts\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dXnjU5nZ0T5F"},"source":["We can see that the non-linear projection does help separate some of the classes better than PCA. Play around with the parameters to get a better result!"]},{"cell_type":"markdown","metadata":{"id":"VJDXXIAH0T5F"},"source":["### UMAP\n","\n","UMAP (Uniform Manifold Approximation and Projection) is a more sophisticated model based on solid mathematical principles. They are fairly sophisticated, so if you want to check them out in detail, read the [original paper](https://arxiv.org/abs/1802.03426). In particular, Appendix C is of great use if you are familiar with Machine Learning notation.\n","\n","UMAP is far more efficient than t-SNE, particularly when projecting into more than two dimensions, so it is generally a better method than t-SNE. It is, however, still not very mainstream.\n","\n","Let's create a 2D projection of our data using UMAP, that works great with sparse matrices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5PBI3h_0T5F","ExecuteTime":{"start_time":"2024-03-20T13:53:15.607396Z","end_time":"2024-03-20T13:53:18.376167Z"}},"outputs":[],"source":["# Let's create the object\n","reducer = umap.UMAP(n_neighbors=15,              # Number of neareast neighbours to use.\n","                    n_components=2,              # Number of components. UMAP is robust to larger values\n","                    metric='hellinger',          # Metric to use.\n","                    n_epochs=None,               # Iterations. Set to convergence. None implies either 200 or 500.\n","                    min_dist=0.1,                # Minimum distance embedded points. Smaller makes clumps, larger, sparseness.\n","                    spread=1.0,                  # Scale to combine with min_dist\n","                    low_memory=True,             # Run slower, but with less memory.\n","                    n_jobs=-1,                   # Cores to use\n","                    random_state=seed,           # Random seed\n","                    verbose=False                # Verbosity\n","                   )\n","\n","# Now we train and calculate the embedding!\n","UMAP_embedding ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxaZbk9g0T5G","ExecuteTime":{"start_time":"2024-03-20T13:53:18.375170Z","end_time":"2024-03-20T13:53:18.561438Z"}},"outputs":[],"source":["# Create plot\n","sns.scatterplot(x=UMAP_embedding[:, 0], y=UMAP_embedding[:, 1], hue=texts['class'])\n","plt.xlabel(\"Component 1\")\n","plt.ylabel(\"Component 2\")\n","plt.title(\"UMAP of IMDB texts\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Bc722DCj0T5G"},"source":["Or alternatively (and using way less memory) we can simply plot the mapper directly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtsJFhUg0T5H","ExecuteTime":{"start_time":"2024-03-20T13:52:51.148455Z","end_time":"2024-03-20T13:52:51.239385Z"}},"outputs":[],"source":["umap.plot.points(reducer, labels=texts['class'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Yib8QsLh0T5H"},"source":["UMAP and t-SNE create much sparser divisions, and one that clearly separates both classes! What we can infer is that there is a significant non-linear separation between these two movie reviews, and that this can be correctly interpreted using non-linear models."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"}}},"nbformat":4,"nbformat_minor":0}