{"cells":[{"cell_type":"markdown","metadata":{"id":"fkAzsH9fHRFm"},"source":["# Global Toolbox"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5347,"status":"ok","timestamp":1678730158636,"user":{"displayName":"Alireza Fazeli","userId":"13019604121988176443"},"user_tz":240},"id":"80XLgR3xHRFp","outputId":"1c2ea5c8-1348-41b3-8133-c2fae9e4ae1a"},"outputs":[],"source":["import warnings; warnings.filterwarnings('ignore')\n","\n","import numpy as np; seed = 356; np.random.seed(seed)\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import seaborn as sns\n","import sklearn\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n","from IPython.display import Image\n","import pandas as pd\n","from functools import reduce\n","# !pip install xgboost\n","from xgboost import XGBClassifier\n","# !pip install gdown\n","import gdown\n","# !pip install pydotplus\n","import pydotplus \n","from tqdm import tqdm\n","\n","print('The version of my sklearn package is', sklearn.__version__)"]},{"cell_type":"markdown","metadata":{"id":"P8sa9v8IHRFr"},"source":["# Part 1: Basis Expansion\n","$f(X) = \\sum_{m=1}^{M} \\beta_m h_m (X)$\n","### Piecewise fitting using conditional basis (with two knots _i.e._, $\\varepsilon_{1}$ and $\\varepsilon_{2}$)\n"]},{"cell_type":"markdown","metadata":{"id":"Wue3q9NgHRFs"},"source":["### Initializing coordinates and regions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HrciAevHRFt"},"outputs":[],"source":["# Axes limits and knots\n","x_min, x_knot_1, x_knot_2, x_max = -1, 1.5, 4.5, 7\n","\n","# Points generated from a cosine function with normal noise\n","x_true = np.linspace(x_min, x_max,50)\n","y_true = np.cos(x_true)\n","y_obs  = y_true + np.random.normal(scale=0.5, size=len(x_true))\n","\n","# Get x-y coordinates per region\n","x_region_1 = x_true[x_true <= x_knot_1]\n","x_region_2 = x_true[(x_knot_1 < x_true) & (x_true < x_knot_2)]\n","x_region_3 = x_true[x_true >= x_knot_2]\n","y_region_1 = y_true[x_true <= x_knot_1]\n","y_region_2 = y_true[(x_knot_1 < x_true) & (x_true < x_knot_2)]\n","y_region_3 = y_true[x_true >= x_knot_2]"]},{"cell_type":"markdown","metadata":{"id":"et78HY3lHRFu"},"source":["### Piecewise constant fitting with the following basis functions:\n","\n","$h_{1}(X)=I(X<\\varepsilon_{1})\\\\$\n","$h_{2}(X)=I(\\varepsilon_{1}\\leq X < \\varepsilon_2 )\\\\$\n","$h_{3}(X)=I(\\varepsilon_{2}\\leq X)\\\\$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-s4bUWRHRFu","outputId":"c133d371-611d-4677-c979-63228d79dc05"},"outputs":[],"source":["# Plot cosine line and noisy data\n","plt.figure(figsize=(4, 3))\n","plt.plot(x_true, y_true, linewidth=3, c='g',  label='True')\n","plt.scatter(x_true, y_obs, label='True+Noise')\n","\n","# Plot knots\n","plt.axvline(x=x_knot_1, c='gray', ls='--', label='Knots')\n","plt.axvline(x=x_knot_2, c='gray', ls='--')\n","\n","# Plot piecewise constant fits\n","plt.axhline(y=y_region_1.mean(), c='r', xmin=0, xmax=0.33, label='Fit')\n","plt.axhline(y=y_region_2.mean(), c='r', xmin=0.33, xmax=0.66)\n","plt.axhline(y=y_region_3.mean(), c='r', xmin=0.66, xmax=1)\n","plt.title('Piecewise Constant')\n","plt.legend(bbox_to_anchor=(1, 1.03))\n","plt.xlabel('X')\n","plt.ylabel('$f(X)$', rotation=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-iAqEfFgHRFv"},"source":["### Piecewise linear fitting with the following basis functions:\n","\n","$h_{1}(X)=I(X<\\varepsilon_{1})\\\\$\n","$h_{2}(X)=I(\\varepsilon_{1}\\leq X < \\varepsilon_2 )\\\\$\n","$h_{3}(X)=I(\\varepsilon_{2}\\leq X)\\\\$\n","$h_{4}(X)=h_1(X)X\\\\$\n","$h_{5}(X)=h_2(X)X\\\\$\n","$h_{6}(X)=h_3(X)X\\\\$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_HyntccHRFw","outputId":"43b2d366-0ceb-4d14-bc0b-98df164a4468"},"outputs":[],"source":["# Calculate OLS coefficients from regression anatomy\n","beta_region_1 = ((y_region_1 - y_region_1.mean()).dot(x_region_1) / \n","                (x_region_1**2).sum())\n","beta_region_2 = ((y_region_2 - y_region_2.mean()).dot(x_region_2) / \n","                (x_region_2**2).sum())\n","beta_region_3 = ((y_region_3 - y_region_3.mean()).dot(x_region_3) / \n","                (x_region_3**2).sum())\n","\n","# Calculate regression fitted values\n","y_hat_region_1 = beta_region_1 * x_region_1 + y_region_1.mean()\n","y_hat_region_2 = beta_region_2 * x_region_2 + y_region_2.mean()\n","y_hat_region_3 = beta_region_3 * x_region_3 + y_region_3.mean()\n","\n","# Plot cosine line and noisy data\n","plt.figure(figsize=(4, 3))\n","plt.plot(x_true, y_true, linewidth=3, c='g',label='True')\n","plt.scatter(x_true, y_obs, label='True+Noise')\n","\n","# Plot knots\n","plt.axvline(x=x_knot_1, c='gray', ls='--', label='Knots')\n","plt.axvline(x=x_knot_2, c='gray', ls='--')\n","\n","# Plot piecewise linear fits\n","plt.plot(x_region_1, y_hat_region_1, c='r', label='Fit')\n","plt.plot(x_region_2, y_hat_region_2, c='r')\n","plt.plot(x_region_3, y_hat_region_3, c='r')\n","plt.title('Piecewise Linear')\n","plt.legend(bbox_to_anchor=(1, 1.03))\n","plt.xlabel('X')\n","plt.ylabel('$f(X)$', rotation=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aiUh4CU1HRF6"},"source":["### Continuous piecewise linear fitting with the following basis functions:\n","\n","$h_{1}(X)=1\\\\$\n","$h_{2}(X)=X\\\\$\n","$h_{3}(X)=(X-\\varepsilon_1)_+\\\\$\n","$h_{4}(X)=(X-\\varepsilon_2)_+\\\\$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCKJUhJ5HRF6","outputId":"614cce94-d56d-401c-fea6-6346345bfc79"},"outputs":[],"source":["# Construct H\n","# Continuity is enforced at the knots through incorporating proper constraints into the basis functions\n","h1 = np.ones_like(x_true)\n","h2 = np.copy(x_true)\n","h3 = np.where(x_true < x_knot_1, 0, x_true - x_knot_1)\n","h4 = np.where(x_true < x_knot_2, 0, x_true - x_knot_2)\n","H  = np.vstack((h1, h2, h3, h4)).T\n","\n","# Fit basis expansion via OLS\n","HH = H.T @ H\n","beta = np.linalg.solve(HH, H.T @ y_obs)\n","y_hat = H @ beta\n","\n","# Plot cosine line and noisy data\n","plt.figure(figsize=(4, 3))\n","plt.plot(x_true, y_true, linewidth=3, c='g', label='True')\n","plt.scatter(x_true, y_obs, label='True+Noise')\n","\n","# Plot knots\n","plt.axvline(x=x_knot_1, c='gray', ls='--', label='Knots')\n","plt.axvline(x=x_knot_2, c='gray', ls='--')\n","\n","# Plot piecewise linear fits\n","plt.plot(x_true, y_hat, c='r', label='Fit')\n","plt.title('Continuous Piecewise Linear')\n","plt.legend(bbox_to_anchor=(1, 1.03))\n","plt.xlabel('X')\n","plt.ylabel('$f(X)$', rotation=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6C35malNHRF7"},"source":["### Continuous piecewise polynomial fitting:\n","Piecewise polynomials, even those continuous at the knots, tend not to be smooth: they rapidly change the slope at the knots. To prevent this and increase their smoothness, it is enough to increase the order of the local polynomial and require the first two derivatives on both sides of the knot to be the same. A function that is continuous and has continuous first and second derivatives is called a cubic spline and can be represented with the following basis functions:\n","\n","$h_{1}(X)=1\\\\$\n","$h_{2}(X)=X\\\\$\n","$h_{3}(X)=X^2\\\\$\n","$h_{4}(X)=X^3\\\\$\n","$h_{5}(X)=(X-\\varepsilon_1)_+^3\\\\$\n","$h_{6}(X)=(X-\\varepsilon_2)_+^3\\\\$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pyyd6Q0wHRF8","outputId":"1fce9351-e81e-4fd2-b1a3-045a2970649e"},"outputs":[],"source":["# Construct H\n","h1 = np.ones_like(x_true) # `ones_like` returns an array of ones with the same shape and type as `x_true`\n","h2 = np.copy(x_true)\n","h3 = h2 ** 2\n","h4 = h2 ** 3\n","h5 = np.where(x_true < x_knot_1, 0, (x_true - x_knot_1) ** 3)\n","h6 = np.where(x_true < x_knot_2, 0, (x_true - x_knot_2) ** 3)\n","H  = np.vstack((h1, h2, h3, h4, h5, h6)).T\n","\n","# Fit basis expansion via OLS\n","HH = H.T @ H\n","beta = np.linalg.solve(HH, H.T @ y_true)\n","y_hat = H @ beta\n","\n","# Plot simulated data and cubic spline\n","plt.figure(figsize=(4, 3))\n","plt.plot(x_true, y_obs, 'o', label='True+Noise')\n","plt.plot(x_true, y_true, linewidth=3, color='g', label='True')\n","plt.plot(x_true, y_hat, color='r', label='Fit')\n","plt.title('Continuous Piecewise Polynomial')\n","\n","# Plot knots\n","plt.axvline(x=x_knot_1, c='gray', ls='--', label='Knots')\n","plt.axvline(x=x_knot_2, c='gray', ls='--')\n","plt.legend(bbox_to_anchor=(1, 1.03))             \n","plt.xlabel('X')\n","plt.ylabel('$f(X)$', rotation=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yhKj9lKwHRF8"},"source":["# Part 2: Decision Trees\n","\n","Review Scikit-Learn [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"elapsed":1713,"status":"ok","timestamp":1678730171938,"user":{"displayName":"Alireza Fazeli","userId":"13019604121988176443"},"user_tz":240},"id":"FXQtU2cQHRF9","outputId":"9d611227-c20c-4ccd-d66d-bbb8c68aeb9b"},"outputs":[],"source":["# Make some fake data\n","def make_regression_data(n=25):\n","    x = np.random.uniform(size = (n,1))\n","    e = np.random.normal(0, 0.3, size = x.shape) # some noise\n","    y = np.sin(2*np.pi*x) + e\n","    return (x, y.ravel())\n","\n","x,y = make_regression_data()\n","fig, ax = plt.subplots(dpi = 120)\n","plt.scatter(x,y)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":639},"executionInfo":{"elapsed":2272,"status":"ok","timestamp":1678730176542,"user":{"displayName":"Alireza Fazeli","userId":"13019604121988176443"},"user_tz":240},"id":"SWtj2JhtHRF9","outputId":"c027998a-cc03-4332-939e-d389c7abe7ea"},"outputs":[],"source":["# Fit a regression tree to this data\n","# With varying max depth\n","# To see how this tree grows\n","\n","depths = np.arange(1,7)\n","fig, ax = plt.subplots(dpi = 120, nrows = 2, ncols = 3, figsize = (10,6))\n","plt.subplots_adjust(hspace=0.25)\n","ax = ax.ravel() #flattens the ax variable\n","\n","newx = np.linspace(0,1,101).reshape(-1,1)\n","\n","for d,a in zip(depths, ax):\n","    reg = DecisionTreeRegressor(max_depth=d)\n","    reg.fit(x,y)\n","    \n","    ypred = reg.predict(newx)\n","    a.scatter(x,y)\n","    a.plot(newx, ypred, color = 'red')\n","    a.set_title(f'max_depth = {d}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":630},"executionInfo":{"elapsed":728,"status":"ok","timestamp":1678730179475,"user":{"displayName":"Alireza Fazeli","userId":"13019604121988176443"},"user_tz":240},"id":"yWPPkwDSHRF9","outputId":"94649c99-aec4-41f3-996c-faf3cc1ce53b"},"outputs":[],"source":["# Create DOT data\n","dot_data = export_graphviz(reg, out_file=None, rounded = True, max_depth=3, feature_names=['x'])\n","\n","# Draw graph\n","graph = pydotplus.graph_from_dot_data(dot_data)  \n","\n","# Show graph\n","Image(graph.create_png())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1HPo9SCHRF-","outputId":"33176b09-504d-4b4d-b7b2-a0af8f909cc9"},"outputs":[],"source":["# Let's compare variance of predictions between a tree and a bagged estimator\n","\n","# Create a tree model and a bagging from these trees\n","newx = np.linspace(0,1,101).reshape(-1,1)\n","tree = DecisionTreeRegressor(max_depth = 5)\n","bag1 = BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth = 5), n_estimators=100, n_jobs = -1)\n","\n","models = [tree, bag1]\n","model_names = ['Tree', \"Bagged\"]\n","\n","# Create two plots, one for the tree, one for the bagging\n","fig, ax = plt.subplots(dpi = 120, nrows = 1, ncols = len(models), figsize = (10,3), sharey = True)\n","ax = ax.ravel() #flattens the ax variable\n","\n","# Generate 500 runs for each model and calculate bias and variance. This takes a while!\n","nsim = 50\n","\n","for axis, model, name in zip(ax, models, model_names):\n","    \n","    #Store the predictions somewhere\n","    predictions = np.zeros((nsim, newx.shape[0]))\n","    \n","    for i in range(nsim):\n","        x,y = make_regression_data()\n","        model.fit(x,y)\n","        ypred = model.predict(newx)\n","        predictions[i] = ypred\n","        \n","    #Plot the variance\n","    axis.plot(newx, predictions.var(axis = 0), label = 'Variance')\n","    \n","    #Plot the bias\n","    bias = (predictions - np.sin(2*np.pi*newx.T)).mean(axis = 0)**2\n","    axis.plot(newx, bias, color = 'red', label = 'Squared Bias')\n","    axis.set_title(name)\n","    axis.legend()"]},{"cell_type":"markdown","metadata":{"id":"RvAL2RUIHRF-"},"source":["The following cell is just the expanded version of the previous cell to make it easier to follow, otherwise they do the same thing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkB5VDcVHRF-","outputId":"29be301f-6bea-44fa-cdf5-5aaf31dcde42"},"outputs":[],"source":["# Let's compare variance of predictions between a tree and a bagged estimator\n","newx = np.linspace(0,1,101).reshape(-1,1)\n","\n","# Generate `nsim` runs for each model and calculate bias and variance. This takes a while!\n","nsim = 50\n","\n","# Create two plots, one for the tree, one for the bagging\n","fig, ax = plt.subplots(dpi=120, nrows=1, ncols=len(models), figsize=(10,3), sharey=True)\n","\n","\n","################################################### 1st figure\n","# Create a simple tree model\n","tree = DecisionTreeRegressor(max_depth=5)\n","# Initialize an array for predictions\n","predictions = np.zeros((nsim, newx.shape[0]))\n","\n","for i in range(nsim):\n","    x, y = make_regression_data() # every time get new data from the same data generating process\n","    tree.fit(x,y)\n","    ypred = tree.predict(newx) # here ypred is coming directly from the terminal \n","                               # nodes of the single tree fitted to x and y\n","    predictions[i] = ypred\n","    \n","#Plot the variance\n","ax[0].plot(newx, predictions.var(axis = 0), label = 'Variance')\n","\n","#Plot the bias\n","bias = (predictions-np.sin(2*np.pi*newx.T)).mean(axis = 0)**2\n","ax[0].plot(newx, bias, color = 'red', label = 'Squared Bias')\n","ax[0].set_title('Tree')\n","\n","\n","################################################### 2nd figure\n","# Create a bagging model\n","bag = BaggingRegressor(DecisionTreeRegressor(max_depth=5), n_estimators=100, n_jobs=-1)\n","# Initialize an array for predictions\n","predictions = np.zeros((nsim, newx.shape[0]))\n","\n","for i in range(nsim):\n","    x, y = make_regression_data() # every time get new data from the same data generating process\n","    bag.fit(x,y)\n","    ypred = bag.predict(newx) # here ypred is coming from averaging individual predictions of\n","                              # random subsets of the original dataset (one tree per random subset)\n","    predictions[i] = ypred\n","    \n","#Plot the variance\n","ax[1].plot(newx, predictions.var(axis = 0), label = 'Variance')\n","\n","#Plot the bias\n","bias = (predictions-np.sin(2*np.pi*newx.T)).mean(axis=0)**2\n","ax[1].plot(newx, bias, color = 'red', label = 'Squared Bias')\n","ax[1].set_title('Bagged')\n","ax[1].legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"no_WhPBqHRF_"},"source":["# Part 3: Random Forest\n","\n","Now we will train a random forest. It is included in the ```sklearn.ensemble``` subpackage, function [```RandomForestClassifier```](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), so it is straightforward to use. It comes with many parameters, but in general there is a philosophy to follow:\n","\n","- In a Random Forest we want each tree to be large, and to learn as much as possible from its subset of data. We don't care too much if each tree is overadjusted, as we can always increase the number of trees to take care of this.\n","\n","- This said, a good idea is to limit the minimum number of samples per leaf when we have few cases (this is not usually a problem in large trees.)\n","\n","- We might want to limit the minimum impurity decrease to stop growing a tree if not much is happening.\n","\n","- There is also a class weight to include. It does include one automatically if we use the option ```balanced```.\n","\n","Let's train one and check the options."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYXXw8K2HRF_"},"outputs":[],"source":["#Define the classifier\n","bankloan_rf = \n","\n","# These are parameters to limit the growth of the tree. If you need to constrain the growth of the tree, pick one of them.\n","# max_depth\n","# min_samples_split\n","# min_samples_leaf  \n","# min_weight_fraction_leaf\n","# min_impurity_decrease  # this is more agnostic to the sample size. If you really need to constrain growth, this is preferred "]},{"cell_type":"markdown","metadata":{"id":"LR5hbZbZHRF_"},"source":["Now we are ready to train. I have created a credit risk dataset that can be used to predict the probability of not paying back a loan (a credit score). We will also split the data into a train and test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvFi0ChGHRF_"},"outputs":[],"source":["# Read the file\n","bankloan_data = \n","\n","# Drop a categorical variable\n","bankloan_data."]},{"cell_type":"markdown","metadata":{"id":"VgHI19ugHRGA"},"source":["The data has the following variables:\n","\n","- Customer: ID, or unique label, of the borrower (NOT predictive).\n","-    Age: Age of the borrower in years.\n","-    Employ: Years at current job.\n","-    Address: Years at current address.\n","-    Income: Income in ‘000s USD.\n","-    Leverage: Debt/Income Ratio.\n","-    CredDebt: Credit card standing debt.\n","-    OthDebt: Other debt in ‘000s USD.\n","-    MonthlyLoad: Monthly percentage from salary used to repay debts.\n","-    Default: 1 If default has occurred, 0 if not (Target variable).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4O5WwyIHRGA","outputId":"8d48f209-1b7d-40f5-f5ad-3f5e1936db08"},"outputs":[],"source":["print(bankloan_data.shape)\n","bankloan_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZ0Asc9uHRGA","outputId":"8f74c988-486f-48e2-91a3-51f50b74dbea"},"outputs":[],"source":["bankloan_data."]},{"cell_type":"markdown","metadata":{"id":"2VD9wc4jHRGB"},"source":["Now we split the data into train and test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfyQORY0HRGB"},"outputs":[],"source":["# Split into train and test, fixing seed.\n","bankloan_train_noWoE, bankloan_test_noWoE = "]},{"cell_type":"markdown","metadata":{"id":"PrIi2ndaHRGB"},"source":["Now we train!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJKufmQTHRGB","outputId":"3ff235b1-8cc1-49cd-a703-01e88c63786f"},"outputs":[],"source":["# Train the RF\n","bankloan_rf."]},{"cell_type":"markdown","metadata":{"id":"NLV1mZMfHRGB"},"source":["We can see it used two jobs (two processors are available to me in this computer). It converges very quickly. Let's check how it did, this time we will print a nicer confusion matrix using seaborn, and will plot the ROC curve of the model. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeqKJ2F5HRGC","outputId":"1990db67-55b9-48a9-d365-a31ed65a6b1b"},"outputs":[],"source":["# Apply the model to the test set.\n","rf_pred_class_test = bankloan_rf.\n","rf_probs_test = bankloan_rf."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NII76UUFHRGC","outputId":"1d2c298c-5daf-46e0-884a-478aabfce4dd"},"outputs":[],"source":["# Calculate confusion matrix\n","confusion_matrix_rf = \n","\n","# Turn matrix to percentages\n","confusion_matrix_rf = \n","\n","# Turn to dataframe\n","df_cm = pd.DataFrame(confusion_matrix_rf, index=['Good', 'Bad'], columns=['Good', 'Bad'])\n","\n","# Parameters of the image\n","figsize = (10,7)\n","fontsize=14\n","\n","# Create image\n","fig = plt.figure(figsize=figsize)\n","heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n","\n","# Make it nicer\n","heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0,  ha='right', fontsize=fontsize)\n","heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","\n","# Add labels\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","\n","# Plot!\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0HlUSijmHRGD"},"source":["[Out-of-Bag (OOB) error](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html)\n","\n","The `RandomForestClassifier` is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations $z_i=(x_i,y_i)$. The out-of-bag (OOB) error is the average error for each $z_i$ calculated using predictions from the trees that do not contain $z_i$ in their respective bootstrap sample. This allows the `RandomForestClassifier` to be fit and validated whilst being trained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-vI3CM3HRGD","outputId":"56a940b1-24b1-41ee-c921-0eff92459483"},"outputs":[],"source":["bankloan_rf\n","# can think of this as validation score"]},{"cell_type":"markdown","metadata":{"id":"uYWwQ-kOHRGD"},"source":["Return the mean accuracy on the given test data and labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQhMxBuTHRGD","outputId":"de8e17c7-a660-4176-c2e8-6b217982d949"},"outputs":[],"source":["bankloan_rf."]},{"cell_type":"markdown","metadata":{"id":"Sx2JnXeLHRGD"},"source":["Looks a bit unbalanced, but otherwise ok. It's harder to predict the  defaulters. Now let's see the ROC curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIKvdY-uHRGE","outputId":"f8ec2a0f-885b-42a9-8309-f78b8ba85713"},"outputs":[],"source":["# Calculate the ROC curve points\n","fpr, tpr, thresholds = \n","\n","# Save the AUC in a variable to display it. Round it first\n","auc = \n","\n","# Create and show the plot\n","plt.plot(fpr,tpr,label=\"Bankloan RF, auc=\"+str(auc))\n","plt.legend(loc=4)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IvPvCq2GHRGE"},"source":["Now, let's print the variable importance. The importance is calculated by averaging the accuracy of trees when the variables is included the tree, and comparing it to when it's NOT included the tree.\n","\n","[Feature Importances with a Forest of Trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n","also [this](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QYnHnm5HRGE","outputId":"ab125b9a-9e3d-4a65-db7a-a21e5da2fc7d"},"outputs":[],"source":["importances = \n","print(bankloan_rf.feature_names_in_)\n","print(importances)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2S6irG9HRGE","outputId":"19e974e1-ae61-4a92-a30c-5134e0d40714"},"outputs":[],"source":["# Plot variable importance\n","indices = np.argsort(importances)[::-1] \n","\n","f, ax = plt.subplots(figsize=(3, 8))\n","\n","plt.title(\"Variable Importance - Random Forest\")\n","sns.set_color_codes(\"pastel\")\n","\n","sns.barplot(y=[bankloan_rf.feature_names_in_[i] for i in indices], x=importances[indices], label=\"Total\", color=\"b\")\n","# or\n","# sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], label=\"Total\", color=\"b\")\n","\n","ax.set(ylabel=\"Attribute\", xlabel=\"Attribute Importance (Entropy)\")\n","sns.despine(left=True, bottom=True)"]},{"cell_type":"markdown","metadata":{"id":"1HixCEenHRGE"},"source":["Also take a look at [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."]},{"cell_type":"markdown","metadata":{"id":"zJe6hxnGHRGF"},"source":["# Part 4: [XGBoosting](https://xgboost.readthedocs.io/en/stable/)\n","\n","XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework and is an alternative to Random Forest. Now we want to create a series of small trees, which will be poorer in individual performance, but together they will be stronger. Training an XGBoost model is harder, because we need to control the model so it creates small trees, but it performs better in small data, something Random Forests do not necessarily accomplish.\n","\n","[XGBClassifier Scikit-Learn API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)\n","\n","[XGBRegressor Scikit-Learn API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)\n","\n","While scikit-learn does have its own implementation of XGB ([```sklearn.ensemble```](https://scikit-learn.org/stable/modules/ensemble.html)), there are a couple of very strong packages out there that implement the algorithm. ```xgboost``` and ```lightgbm``` are two of the best known ones. We will use ```xgboost``` for this lab, available pretty much for every language out there.\n","\n","The first step is to define a classifier that we will use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFhTbXzpHRGF"},"outputs":[],"source":["#Define the classifier.\n","\n","# Specify the learning task and the corresponding learning objective. The objective options are below:\n","# https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters\n","\n","XGB_Bankloan = XGBClassifier("]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ON22HftKHRGF","outputId":"d07a0a66-91e2-4390-d557-341a75ee81a7"},"outputs":[],"source":["bankloan_data.Default.value_counts()\n","\n","# # we are interested in the number of default cases --> positive class\n","# print(bankloan_data.Default.sum())\n","\n","# # number of non-default cases --> negative class\n","# print(bankloan_data.shape[0]-bankloan_data.Default.sum())"]},{"cell_type":"markdown","metadata":{"id":"rb_lssaZHRGF"},"source":["This classifier can be used to tune the parameters of the model. We will use sklearn's  [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for this. It requires a dictionary of the parameters to look for. We will tune the number of trees (XGB overfits relatively easily, always tune this), the depth, and the learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqtGLLIHHRGG","outputId":"e5ca520c-78b4-447d-97e7-ae16ad55d511"},"outputs":[],"source":["# Define the parameters. Play with this grid!\n","param_grid = \n","param_grid"]},{"cell_type":"markdown","metadata":{"id":"FqRromI-HRGG"},"source":["Recap: [Stratified Sampling](https://en.wikipedia.org/wiki/Stratified_sampling)"]},{"cell_type":"markdown","metadata":{"id":"AVtMAlrpHRGG"},"source":["We will create a validation set for the sample."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCB87-FsHRGH"},"outputs":[],"source":["# Always a good idea to tune on a reduce sample of the train set, as we will call many functions.\n","val_train = \n","\n","\n","# Crossvalidation object\n","cv_object = "]},{"cell_type":"markdown","metadata":{"id":"aqy-UXv8HRGH"},"source":["Now we can do a [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) over the parameter space. We will use the AUC (as this is a binary classification problem)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Me4LQO3HRGH"},"outputs":[],"source":["# Define grid search object.\n","GridXGB = "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-qu-adEHRGH","outputId":"70076a94-0554-467f-8022-353e5200b324"},"outputs":[],"source":["# Train grid search.\n","GridXGB."]},{"cell_type":"markdown","metadata":{"id":"zyP4gfCAHRGH"},"source":["Now we can output the optimal parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_MLIU6nHRGI","outputId":"11a1b887-4551-4f15-86ab-6860c6112a93"},"outputs":[],"source":["# Show best params\n","GridXGB."]},{"cell_type":"markdown","metadata":{"id":"9z_s9GLMHRGI"},"source":["It is telling us to use 10% learning rate with a max_depth of 2 and 50 trees. If any parameter appears to be at the upper limit, you would need to run it again with a higher limit.\n","\n","We can get the details of the runs accessing cv_results_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S02K2tCGHRGI","outputId":"5d2ab600-b11a-483b-c536-491ca1c593de"},"outputs":[],"source":["GridXGB."]},{"cell_type":"markdown","metadata":{"id":"ksSUbjAPHRGI"},"source":["Now we can fit the final model!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hzpPT7OHRGJ"},"outputs":[],"source":["# Let's recreate XGB with the best parameters:\n","XGB_Bankloan = XGBClassifier(max_depth=          # Depth of each tree\n","                             learning_rate=      # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n","                             n_estimators=       # How many trees to use, the more the better, but decrease learning rate if many used.\n","                             verbosity=1,                  # If to show more errors or not.\n","                             objective='binary:logistic',  # Type of target variable.\n","                             booster='gbtree',             # What to boost. Trees in this case.\n","                             n_jobs=4,                     # Parallel jobs to run. Set your processor number.\n","                             gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n","                             subsample=0.632,              # Subsample ratio. Can set lower\n","                             colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n","                             colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n","                             colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n","                             reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n","                             reg_lambda=0,                 # Regularizer for first fit.\n","                             scale_pos_weight=1,           # Balancing of positive and negative weights.\n","                             base_score=0.5,               # Global bias. Set to average of the target rate.\n","                             random_state=seed\n","                            )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVNiATG3HRGJ","outputId":"16fc36da-aff2-40e7-ee16-07153e7ea4f9"},"outputs":[],"source":["# Train over all training data.\n","XGB_Bankloan."]},{"cell_type":"markdown","metadata":{"id":"uJ2A8iC3HRGJ"},"source":["Now we can evaluate our model. First we calculate the variable importance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjFxsdauHRGJ","outputId":"fa26b391-dd59-460f-abcf-9a32f74b00ef"},"outputs":[],"source":["# Plot variable importance\n","importances = \n","indices = \n","\n","f, ax = plt.subplots(figsize=(3, 8))\n","plt.title(\"Variable Importance - XGBoosting\")\n","sns.set_color_codes(\"pastel\")\n","sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n","            label=\"Total\", color=\"b\")\n","ax.set(ylabel=\"Variable\",\n","       xlabel=\"Variable Importance (Gini)\")\n","sns.despine(left=True, bottom=True)"]},{"cell_type":"markdown","metadata":{"id":"un2v4MIlHRGJ"},"source":["What do you see here? Does it make sense to you?\n","\n","When the correlation between the variables are high, XGBoost will pick one feature and may use it while breaking down the tree and it will ignore some/all the other remaining correlated features (because we will not be able to learn different aspects of the model by using these correlated feature because it is already highly correlated with the chosen feature). But in random forest , the tree is not built from specific features, rather there is random selection of features, and then the model in whole learn different correlations of different features. So you can see the procedure of two methods are different so you can expect to see different feature importance reports. Also, one needs to check default arguments of each method for calculation of FI. For example, XGBoost's default measure is average gain whereas it is total gain in sklearn. "]},{"cell_type":"markdown","metadata":{"id":"Vn1Z8tPbHRGJ"},"source":["Let's finish by plotting the ROC curve. How does it compare to Random Forest? Why do you think this is?\n","\n","In general, boosting methods work better for small datasets with fewer features.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vMz9hPIHRGK","outputId":"5e17ca7f-82a8-49a7-b59d-5fee6603d1e4"},"outputs":[],"source":["# Calculate probability\n","probTest = \n","probTest = \n","\n","# Calculate the ROC curve points\n","fpr, tpr, thresholds = \n","\n","# Save the AUC in a variable to display it. Round it first\n","auc = \n","\n","# Create and show the plot\n","plt.plot(fpr,tpr,label=\"AUC - XGBoosting = \" + str(auc))\n","plt.legend(loc=4)\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.10 ('my_env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"}}},"nbformat":4,"nbformat_minor":0}
