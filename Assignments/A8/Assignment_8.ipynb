{"cells":[{"cell_type":"markdown","id":"c1130c0e","metadata":{},"source":["# Assignment 08: \n","# Text Mining and Regression using Dimensionality Reduction Methods [_/100 Marks]\n","\n","### Follow These Instructions\n","\n","Once you are finished, ensure to complete the following steps.\n","\n","1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n","\n","2.  Fix any errors which result from this.\n","\n","3.  Repeat steps 1. and 2. until your notebook runs without errors.\n","\n","4.  Submit your completed notebook to OWL by the deadline.\n","\n","\n","\n","#### In this assignment, we will study apply dimensionality reduction methods to improve our understanding of text data and to predict the sentiment of a set of texts. The dataset for this assignment comes from the Amazon website and represents 1,000 reviews which were labeled (by humans) as positive or negative. This application of data science is called [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) and it is widely used across many fields to get automated feedback when text opinions are expressed. You will also work with dimensionality reduction for classification and regression.\n","---"]},{"cell_type":"code","execution_count":24,"id":"respective-contrast","metadata":{"executionInfo":{"elapsed":108,"status":"ok","timestamp":1647454126987,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"respective-contrast"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import umap\n","from sklearn.decomposition import PCA, TruncatedSVD\n","import sklearn.feature_extraction.text as sktext\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve, roc_auc_score\n","from itertools import product\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","seed = 221113"]},{"cell_type":"markdown","id":"d5d0f400","metadata":{},"source":["---\n","## Task 1: Decomposition of the texts [ / 50 marks]"]},{"cell_type":"markdown","id":"declared-federal","metadata":{"id":"declared-federal"},"source":["### Question 1.1 [ / 9 marks]\n","\n","The dataset comes with the text and a binary variable which represents the sentiment, either positive or negative. Import the data and use sklearn's `TfidfVectorizer` to eliminate accents, special characters, and stopwords. In addition, make sure to eliminate words that appear in less than 5% of documents and those that appear in over 95%. You can also set `sublinear_tf` to `True`. After that, split the data into train and test with `test_size = 0.2` and `seed = seed`. Calculate the [Tf-Idf transform](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for both train and test. Note that you need to fit and transform the inputs for the train set but you only need to transform the inputs for the test set. Don't forget to turn the sparse matrices to dense ones after you apply the `Tf-Idf` transform.  "]},{"cell_type":"code","execution_count":25,"id":"1f703a98","metadata":{},"outputs":[],"source":["# Load the data [ /1 marks]\n","\n","\n","# Display the first 5 rows [ /1 marks]\n"]},{"cell_type":"code","execution_count":26,"id":"clear-imperial","metadata":{"executionInfo":{"elapsed":495,"status":"ok","timestamp":1647450957891,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"clear-imperial"},"outputs":[],"source":["# Defining the TfIDFTransformer [ /2 marks]\n","\n","\n","# Train/test split [ /2 marks]\n","\n","# Calculate the Tf-Idf transform [ /2 marks]\n"]},{"cell_type":"markdown","id":"bacterial-calvin","metadata":{"id":"bacterial-calvin"},"source":["From here on, you will use the variables `TfIDF_train` and `TfIDF_test` as the input for the different tasks, and the `y_train` and `y_test` labels for each dataset (if required). Print the number of indices in the ouput using [`TfIDFTransformer.get_feature_names()` method](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)."]},{"cell_type":"code","execution_count":27,"id":"continental-batch","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247,"status":"ok","timestamp":1647450961573,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"continental-batch","outputId":"472436de-7712-4629-b7c1-49b73327e980"},"outputs":[],"source":["# Print the number of indices [ /1 marks]\n"]},{"cell_type":"markdown","id":"tribal-scholarship","metadata":{"id":"tribal-scholarship"},"source":["### Question 1.2 [ / 8 marks]\n","Now we have the TfIDF matrix so we can start working on the data. We hope to explore what some commonly occuring concepts are in the text reviews. We can do this using PCA. A PCA transform of the TF-IDF matrix will give us a basis of the text data, each component representing a *concept* or set of words that are correlated. Correlation in text can be interpreted as a relation to a similar topic. Calculate a PCA transform of the training data using the **maximum** number of concepts possible. Make a plot of the explained variance that shows the cumulative explained variance per number of concepts."]},{"cell_type":"code","execution_count":28,"id":"valid-uganda","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":1012,"status":"ok","timestamp":1647450964800,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"valid-uganda","outputId":"bc731e0c-ff31-478b-d921-b69e7ab4eec6"},"outputs":[],"source":["# Apply PCA on training data and get the explained variance [ / 3 marks]\n","\n","\n","# Plotting explained variance with number of concepts [ / 3 marks]\n"]},{"cell_type":"markdown","id":"eeeaebd3","metadata":{},"source":["**Written Question:** Exactly how many concepts do we need to correctly explain at least 80% of the data? [ /2 marks]\n"]},{"cell_type":"code","execution_count":29,"id":"fitted-rogers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1647450967325,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"fitted-rogers","outputId":"fab5395c-1adc-4681-b8ea-3974b5f8e48a"},"outputs":[],"source":["# To get the exact index where the variance is above 80%\n"]},{"cell_type":"markdown","id":"224b4fd3","metadata":{},"source":["**Your Answer:**\n","\n","Here"]},{"cell_type":"markdown","id":"electronic-calgary","metadata":{"id":"electronic-calgary"},"source":["### Question 1.3 [ / 12 marks]\n","\n","Let's examine the first three concepts by looking how many variance they explained and showing the 10 words that are the most important in each of these three concepts (as revealed by the absolute value of the PCA weight in each concept).\n"]},{"cell_type":"code","execution_count":30,"id":"lesser-prison","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1647451058008,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"lesser-prison","outputId":"554312d5-38b1-464b-ff80-f34db78ea91a"},"outputs":[],"source":["# Explained variance [ / 2 marks]\n"]},{"cell_type":"code","execution_count":31,"id":"based-deviation","metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1647451060599,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"based-deviation"},"outputs":[],"source":["# Get 10 most important words for each component [ / 4 marks]\n"]},{"cell_type":"code","execution_count":32,"id":"gorgeous-montana","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1647451061365,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"gorgeous-montana","outputId":"4d80a261-4585-4c03-e387-ab5105717fa3"},"outputs":[],"source":["# Words for concept 1 [ / 2 marks]\n"]},{"cell_type":"code","execution_count":33,"id":"mature-cyprus","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1647451061888,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"mature-cyprus","outputId":"07360eb6-2f39-4503-d3a9-376068ce38f2"},"outputs":[],"source":["# Words for concept 2 [ / 1 marks]\n"]},{"cell_type":"code","execution_count":34,"id":"proprietary-contributor","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647451062389,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"proprietary-contributor","outputId":"31e53ce1-e1fb-427c-9940-3fc69e5d129c"},"outputs":[],"source":["# Words for concept 3 [ / 1 marks]\n"]},{"cell_type":"markdown","id":"ahead-evaluation","metadata":{"id":"ahead-evaluation"},"source":["\n","**Written Question:** What is the cumulative variance explained by these three concepts? What would you name each of these concepts? [ / 2 marks]\n","\n","*Hint: If in a concept you would get the words 'dog', 'cat', 'fish' as the most important ones, you could name the concept 'animals' or 'pets'.*"]},{"cell_type":"markdown","id":"0bed1997","metadata":{},"source":["**Your answer:**\n","\n","Here"]},{"cell_type":"markdown","id":"outstanding-hepatitis","metadata":{"id":"outstanding-hepatitis"},"source":["### Question 1.4 [ / 8 marks]\n","\n"," Apply the PCA transformation to the test dataset. Use only the first two components and make a scatter plot of the cases. Identify positive and negative cases by colouring points with different sentiments with different colours.\n"]},{"cell_type":"code","execution_count":35,"id":"ruled-central","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"elapsed":1061,"status":"ok","timestamp":1647451080085,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"ruled-central","outputId":"f9481b07-9363-4352-b108-755cae54e9ca"},"outputs":[],"source":["# Apply PCA to the test dataset [ / 2 marks]\n","\n","\n","# Plot the two different set of points with different markers and labels [ /4 marks]\n"]},{"cell_type":"markdown","id":"present-sunglasses","metadata":{"id":"present-sunglasses"},"source":["**Written Question:** What can we say about where the positive and negative cases lie in our plot? Could we use these concepts to discriminate positive and negative cases? If yes, why? If no, why not? Discuss your findings. [ /2 marks]"]},{"cell_type":"markdown","id":"8b0819d4","metadata":{},"source":["**Your answer:**\n","\n","Here"]},{"cell_type":"markdown","id":"dt7hcc3noPEU","metadata":{"id":"dt7hcc3noPEU"},"source":["### Question  1.5 [ / 13 marks]\n","\n","Repeat the process above, only now using a UMAP projection with two components. Test all combinations of ```n_neighbors=[2, 10, 25]``` and ```min_dist=[0.1, 0.25, 0.5]``` over the train data and choose the projection that you think is best, and apply it over the test data. Use 1000 epochs, a cosine metric and random initialization. If you have more than 8GB of RAM (as in Colab), you may want to set ```low_memory=False``` to speed up computations.\n","\n","*Hint: [This link](https://stackoverflow.com/questions/16384109/iterate-over-all-combinations-of-values-in-multiple-lists-in-python) may be helpful.*\n","\n"]},{"cell_type":"code","execution_count":36,"id":"k5s0D9iuoFBT","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":71372,"status":"ok","timestamp":1647453628089,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"k5s0D9iuoFBT","outputId":"beec454e-4b2c-43b0-da19-39918b7363a5"},"outputs":[],"source":["# Set parameters [ / 4 marks]\n","\n","\n","# Create plot [ [ /2 marks]]\n"]},{"cell_type":"markdown","id":"KyMYOAMf4l_E","metadata":{"id":"KyMYOAMf4l_E"},"source":["**Written Question:** Which paramter would you choose? [ / 2 makrs]"]},{"cell_type":"markdown","id":"1d7b0d01","metadata":{},"source":["**Your Answer:**\n","\n","Here"]},{"cell_type":"code","execution_count":37,"id":"-21cSLBty9gh","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":34556,"status":"ok","timestamp":1647453944814,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"-21cSLBty9gh","outputId":"8257709e-2e37-4fc0-d64f-f2c71481a449"},"outputs":[],"source":["# Choose the paramters that you think are best and apply to test set [ / 2 marks]\n","\n","\n","# Create plot [ /1 marks]\n"]},{"cell_type":"markdown","id":"A40NKn9Y5tM9","metadata":{"id":"A40NKn9Y5tM9"},"source":["**Written Question:** How does the plot compare to the PCA one? [ /2 marks]"]},{"cell_type":"markdown","id":"0def5fa4","metadata":{},"source":["**Your answer:**\n","\n","Here"]},{"cell_type":"markdown","id":"nuclear-landing","metadata":{"id":"nuclear-landing"},"source":["---\n","## Task 2: Benchmarking predictive capabilities of the compressed data [ /24 marks]\n","\n","For this task, we will benchmark the predictive capabilities of the compressed data against the original one. \n"]},{"cell_type":"markdown","id":"4137a1ce","metadata":{},"source":["\n","### Question 2.1 [ /6 marks]\n","Train a regularized logistic regression over the original TfIDF train set (with no compression) using l2 regularization. Calculate the AUC score and plot the ROC curve for the original test set. Use the training/test split created in Q1.1."]},{"cell_type":"code","execution_count":38,"id":"frank-madison","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":2272,"status":"ok","timestamp":1647454057484,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"frank-madison","outputId":"73a40232-b7a0-49a6-ba9c-70d71aaf4c2b"},"outputs":[],"source":["# Train and test using model LogisticRegressionCV [ /4 marks]\n","\n","# Define the model\n","\n","\n","# Fit on the training dataset\n","\n","\n","# Apply to the test dataset\n","\n","\n","\n","\n","# Plot ROC curve and compute AUC score [ /2 marks]\n","# Calculate the ROC curve points\n","\n","\n","# Save the AUC in a variable to display it. Round it first\n","\n","\n","# Create and show the plot\n"]},{"cell_type":"markdown","id":"8d2f05ad","metadata":{},"source":["### Question 2.2 [ /8 marks]\n","Train a regularized logistic regression over an SVD-reduced dataset (with 10 components) using l2 regularization. Calculate the AUC score and plot the ROC curve for the SVD-transformed test set."]},{"cell_type":"code","execution_count":39,"id":"descending-picking","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":1143,"status":"ok","timestamp":1647454163137,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"descending-picking","outputId":"a609e4eb-574d-400c-8f58-2edb49283ebe"},"outputs":[],"source":["# Apply SVD first [ / 3 marks]\n","\n","\n","#Train and test using model LogisticRegressionCV [ /3 marks]\n","\n","\n","\n","# Plot ROC curve and compute AUC score [ /2 marks]\n","# Calculate the ROC curve points\n","\n","\n","# Save the AUC in a variable to display it. Round it first\n","\n","\n","# Create and show the plot\n"]},{"cell_type":"markdown","id":"814b2fc4","metadata":{},"source":["### Question 2.3 [ /8 marks]\n","Train a regularized logistic regression over the UMAP-reduced dataset (with 10 components using the same parameters as Task 1.5) using l2 regularization. Calculate the AUC score and plot the ROC curve for the UMAP-transformed test set."]},{"cell_type":"code","execution_count":40,"id":"unnecessary-geology","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":30177,"status":"ok","timestamp":1647454229011,"user":{"displayName":"Cristián Bravo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi6QphwVhivjPBW56t4xTTzZ1QOojjow-Fo5Ib0gA=s64","userId":"07563968753594904197"},"user_tz":240},"id":"unnecessary-geology","outputId":"8825abfc-2a2f-4879-9028-22576bbf0f7b"},"outputs":[],"source":["# Apply UMAP first [ / 3 marks]\n","\n","\n","#Train and test using model LogisticRegressionCV [ /3 marks]\n","\n","\n","# Plot ROC curve and compute AUC score [ /2 marks]\n","# Calculate the ROC curve points\n","\n","\n","# Save the AUC in a variable to display it. Round it first\n","\n","\n","# Create and show the plot\n"]},{"cell_type":"markdown","id":"damaged-peace","metadata":{"id":"damaged-peace"},"source":["### Question 2.4 [ /2 marks]\n","**Written Question:** Compare the performance of the three models. Which one is the best. [ / 2 marks] "]},{"cell_type":"markdown","id":"e06419e2","metadata":{},"source":["**Your Answer:**\n","\n","Here"]},{"cell_type":"markdown","id":"a237808a","metadata":{},"source":["---\n","## Task 3: PCA + Hockey [ / 26 marks]\n","Connor Andrew McDavid is a Canadian professional ice hockey player and captain of the Edmonton Oilers of the National Hockey League (NHL). The data file `Hockey_sample.csv` provides a reduced version of Connor's game by game career data. Each row represents the stats of one game. The dataset has the following attributes:\n","\n","|#| Attribute | Description |\n","| --- | --- | --- |\n","|0|`opposingTeam`|The team the player played against.|\n","|1|`home_or_away`|Whether a game was played home or away.|\n","|2|`icetime`|Log10 of total time the player played in seconds.|\n","|3|`gameScore`|Game score rating.|\n","|4|`I_F_primaryAssists`|Primary Assists the player has received on teammates' goals.|\n","|5|`I_F_secondaryAssists`|Secondary Assists the player has received on teammates' goals.|\n","|6|`log10_I_F_shotAttempts`|Log10 of shot attempts. Includes player's shots on goal, missed shots, and blocked shot attempts.|\n","|7|`I_F_goals`|Number of goals the player scored.|\n","|8|`I_F_rebounds`|Rebound shot attempts. These must occur within 3 seconds of a previous shot.|\n","|9|`I_F_reboundGoals`|Goals from rebound shot attempts.|\n","|10|`I_F_freeze`|Puck freezes after a player's shot. The  number of puck freezes by goalies after the player's unblocked shot attempts.|\n","|11|`I_F_playContinuedInZone`|Number of times the play continues in the offensive zone after the player's shot besides an immediate rebound shot.|\n","|12|`I_F_playContinuedOutsideZone`|Number of times the play goes outside the offensive zone after the player's shot.|\n","|13|`I_F_savedShotsOnGoal`|Number of the player's unblocked shots that were saved by the goalie.|\n","|14|`I_F_savedUnblockedShotAttempts`|Number of the player's unblocked shots that were saved by the goalie or missed the net.|\n","|15|`I_F_penalityMinutes`|Number of penalty minutes the player has received.|\n","|16|`log10_I_F_faceOffsWon`|Log10 of number of faceoffs the player has won.|\n","|17|`I_F_hits`|Number of hits the player has given.|\n","|18|`I_F_takeaways`|Number of takeaways the player has taken from opponents.|\n","|19|`I_F_giveaways`|Number of giveaways the player has given to other team.|\n","|20|`I_F_lowDangerGoals`|Goals from low danger shots.|\n","|21|`I_F_mediumDangerGoals`|Goals from medium danger shots.|\n","|22|`I_F_highDangerGoals`|Goals from high danger shots.|\n","|23|`I_F_unblockedShotAttempts`|All shot attempts that weren't blocked.|\n","|24|`I_F_dZoneGiveaways`|Giveaways in the team's defensive zone.|\n","|25|`penalityMinutesDrawn`|Number of penalty minutes the player has drawn.|\n","|26|`penaltiesDrawn`|Number of penalties the player has drawn.|"]},{"cell_type":"markdown","id":"ef88d217","metadata":{},"source":["### Question 3.1 [ / 6 marks]\n","\n","Drop categorical attributes, standardize numerical ones, and finally, with \"icetime\" as your target create the matrix of predictors and target vector, calling them `X1` and `y`, respectively. What is the `shape` of `X1` and `y`?\n","\n","Hint they should be as the following:\n","\n","Shape of X1: (2725, 24)\n","\n","Shape of y: (2725,)"]},{"cell_type":"code","execution_count":41,"id":"6993f0ca","metadata":{},"outputs":[],"source":["#"]},{"cell_type":"markdown","id":"4f4e0373","metadata":{},"source":["### Question 3.2 [ / 10 marks]\n","\n","Use a 15-component regular PCA to transform `X1` and create the scree plot. Let $p$ be the **minimum** number of PCs required in order to capture at least 80% of total variance. What would be the value of $p$? Reduce the dimension of `X1` to $p$ and call this new array `X2` (retain `X1` intact though, we need it for later)."]},{"cell_type":"code","execution_count":42,"id":"b8b46749","metadata":{},"outputs":[],"source":["#"]},{"cell_type":"markdown","id":"f44e0f6f","metadata":{},"source":["### Question 3.3 [ / 10 marks]\n","\n","Now that you have 2 different design matrices (*i.e.*, `X1` and `X2`) let's try two different scenarios: Train a simple linear regression (with default arguments) once using `X1`, and another time using the combination of `X1` and `X2` (*i.e.* concatenate them). Use cross-validation with RMSE as the error measure to identify the best model among the two. Report the cross-validation RMSE along with thier CIs for both models.\n","\n","(For the cross-validation, do five-fold shuffled. For train/test split, use sklearn's default value for test set size.)"]},{"cell_type":"code","execution_count":43,"id":"d15de1da","metadata":{},"outputs":[],"source":["#"]},{"cell_type":"markdown","id":"04a27b65","metadata":{},"source":["---\n","$$The End$$"]}],"metadata":{"colab":{"name":"Assignment_7_Solutions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 ('my_env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"}}},"nbformat":4,"nbformat_minor":5}
